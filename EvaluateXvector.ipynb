{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2612f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edcb68c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "#test = test.sample(30000, replace=False)\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb58ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55768, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "460ef1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[meta[\"locale\"] != \"kz\", \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" +  meta.loc[meta[\"locale\"] != \"kz\"][\"locale\"] + \"/clips/\" + meta.loc[meta[\"locale\"] != \"kz\"][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n",
    "workdir = \"/tf/datasets/xvector\"\n",
    "\n",
    "#meta[\"target\"] = meta[\"target\"].apply(to_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b6346b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>locale</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1486</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56701</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>56701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3364</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>3364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110475</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>rw</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>110475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45384</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>en</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>45384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30751</th>\n",
       "      <td>2677</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>2677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30752</th>\n",
       "      <td>881</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30753</th>\n",
       "      <td>68709</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>68709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30754</th>\n",
       "      <td>249025</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>en</td>\n",
       "      <td>dev</td>\n",
       "      <td>2</td>\n",
       "      <td>249025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30755</th>\n",
       "      <td>2986</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>uk</td>\n",
       "      <td>dev</td>\n",
       "      <td>3</td>\n",
       "      <td>2986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168971 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               path locale  \\\n",
       "0            1486  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "1           56701  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "2            3364  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "3          110475  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     rw   \n",
       "4           45384  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     en   \n",
       "...           ...                                                ...    ...   \n",
       "30751        2677  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "30752         881  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "30753       68709  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "30754      249025  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     en   \n",
       "30755        2986  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     uk   \n",
       "\n",
       "       split  target      id  \n",
       "0      train       1    1486  \n",
       "1      train       0   56701  \n",
       "2      train       1    3364  \n",
       "3      train       3  110475  \n",
       "4      train       2   45384  \n",
       "...      ...     ...     ...  \n",
       "30751    dev       1    2677  \n",
       "30752    dev       1     881  \n",
       "30753    dev       0   68709  \n",
       "30754    dev       2  249025  \n",
       "30755    dev       3    2986  \n",
       "\n",
       "[168971 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14ea214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-02 11:26:05.074 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.900, 1.100]\n",
      "2021-06-02 11:26:05.243 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-02 11:26:05.272 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-02 11:26:06.118 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-02 11:26:06.142 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-02 11:26:06.987 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-02 11:26:07.013 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.9, max=1.1, flag=None)\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "           #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d22384e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"x-vector\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, 40)]        0         \n",
      "_________________________________________________________________\n",
      "channel_dropout (SpatialDrop (None, None, 40)          0         \n",
      "_________________________________________________________________\n",
      "frame1 (Conv1D)              (None, None, 512)         102912    \n",
      "_________________________________________________________________\n",
      "frame2 (Conv1D)              (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "frame3 (Conv1D)              (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "frame4 (Conv1D)              (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "frame5 (Conv1D)              (None, None, 1500)        769500    \n",
      "_________________________________________________________________\n",
      "stats_pooling (GlobalMeanStd (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "segment1 (Dense)             (None, 512)               1536512   \n",
      "_________________________________________________________________\n",
      "segment2 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "log_softmax (Activation)     (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 4,510,176\n",
      "Trainable params: 4,510,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import lidbox.models.xvector as xvector\n",
    "model_input_type = \"logmelspec\"\n",
    "\n",
    "def create_model(num_freq_bins, num_labels):\n",
    "    model = xvector.create([None, num_freq_bins], num_labels, channel_dropout_rate=0.8)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "        metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    num_freq_bins=20 if model_input_type == \"mfcc\" else 40,\n",
    "    num_labels=4)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e3c45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "639efaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lidbox.util import predict_with_model, classification_report\n",
    "from lidbox.visualize import draw_confusion_matrix\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    model = create_model()\n",
    "    print(os.path.join(cachedir, \"model\", model.name))\n",
    "    model.load_weights(os.path.join(cachedir, \"model\", model.name))\n",
    "    return model\n",
    "\n",
    "\n",
    "def display_classification_report(report):\n",
    "    for m in (\"avg_detection_cost\", \"avg_equal_error_rate\", \"accuracy\"):\n",
    "        print(\"{}: {:.3f}\".format(m, report[m]))\n",
    "\n",
    "    lang_metrics = pd.DataFrame.from_dict(\n",
    "        {k: v for k, v in report.items() if k in lang2target})\n",
    "    lang_metrics[\"mean\"] = lang_metrics.mean(axis=1)\n",
    "    display(lang_metrics.T)\n",
    "\n",
    "    fig, ax = draw_confusion_matrix(report[\"confusion_matrix\"], lang2target)\n",
    "\n",
    "model.load_weights(\"xvector.h5\")\n",
    "\n",
    "def predict_with_ap_loss(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        # Generate language vector for input spectra\n",
    "        language_vector = model(x[\"input\"], training=False)\n",
    "        print(language_vector)\n",
    "        # Predict languages by computing distances to reference directions\n",
    "        return x[\"id\"], model.loss.predict(language_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d36e604",
   "metadata": {},
   "source": [
    "## Test on common voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50420d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_dataframe(ids, predictions):\n",
    "    return (pd.DataFrame.from_dict({\"id\": ids, \"prediction\": predictions})\n",
    "           )\n",
    "\n",
    "def predict_with_model(model, ds, predict_fn=None):\n",
    "    \"\"\"\n",
    "    Map callable model over all batches in ds, predicting values for each element at key 'input'.\n",
    "    \"\"\"\n",
    "    if predict_fn is None:\n",
    "        def predict_fn(x):\n",
    "            with tf.device(\"GPU\"):\n",
    "                return x[\"id\"], model(x[\"input\"], training=False)\n",
    "\n",
    "    ids = []\n",
    "    predictions = []\n",
    "    for id, pred in ds.map(predict_fn, num_parallel_calls=TF_AUTOTUNE).unbatch().as_numpy_iterator():\n",
    "        ids.append(id.decode(\"utf-8\"))\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return predictions_to_dataframe(ids, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed75c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = predict_with_model(\n",
    "    model=model,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29cedc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71684-000001</th>\n",
       "      <td>[0.00038267265, 0.068015866, 0.46493787, 0.466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71684-000002</th>\n",
       "      <td>[0.00036228108, 0.030282797, 0.38672042, 0.582...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71684-000003</th>\n",
       "      <td>[0.00016482621, 0.031329192, 0.3511263, 0.6173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71684-000004</th>\n",
       "      <td>[1.9100436e-05, 0.0064635505, 0.75075495, 0.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88574-000001</th>\n",
       "      <td>[1.0, 3.066749e-09, 2.8158553e-09, 1.636421e-08]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259-000003</th>\n",
       "      <td>[4.999837e-08, 0.9930757, 5.8501515e-05, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000001</th>\n",
       "      <td>[0.00031434407, 0.11699918, 0.03570789, 0.8469...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000002</th>\n",
       "      <td>[4.4270102e-05, 0.061910614, 0.0126767345, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000003</th>\n",
       "      <td>[4.273161e-05, 0.18298917, 0.006036767, 0.8109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000004</th>\n",
       "      <td>[6.305647e-05, 0.35091087, 0.005703782, 0.6433...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175438 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     prediction\n",
       "id                                                             \n",
       "71684-000001  [0.00038267265, 0.068015866, 0.46493787, 0.466...\n",
       "71684-000002  [0.00036228108, 0.030282797, 0.38672042, 0.582...\n",
       "71684-000003  [0.00016482621, 0.031329192, 0.3511263, 0.6173...\n",
       "71684-000004  [1.9100436e-05, 0.0064635505, 0.75075495, 0.24...\n",
       "88574-000001   [1.0, 3.066749e-09, 2.8158553e-09, 1.636421e-08]\n",
       "...                                                         ...\n",
       "259-000003    [4.999837e-08, 0.9930757, 5.8501515e-05, 0.006...\n",
       "12245-000001  [0.00031434407, 0.11699918, 0.03570789, 0.8469...\n",
       "12245-000002  [4.4270102e-05, 0.061910614, 0.0126767345, 0.9...\n",
       "12245-000003  [4.273161e-05, 0.18298917, 0.006036767, 0.8109...\n",
       "12245-000004  [6.305647e-05, 0.35091087, 0.005703782, 0.6433...\n",
       "\n",
       "[175438 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86fa3a2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['id'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-259b6a1744d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchunk2pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk2pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   4553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['id'] are in the columns\""
     ]
    }
   ],
   "source": [
    "chunk2pred = chunk2pred.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecc03036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0005371941, 0.28633153, 0.069509454, 0.6436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.00015652769, 0.20875472, 0.12081046, 0.6702...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[4.5013534e-05, 0.69266313, 0.006293945, 0.300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[0.21455875, 0.36390203, 0.076663665, 0.34487557]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>[1.0598835e-06, 0.0009105328, 0.0023490405, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>[0.9999947, 7.870432e-07, 1.0888429e-07, 4.364...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>[0.9999991, 5.2881205e-08, 4.136213e-08, 8.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>[0.99999946, 3.9046526e-08, 8.0310656e-08, 5.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>[0.9999855, 8.9647085e-07, 1.2998103e-06, 1.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[3.7698182e-06, 0.54326427, 0.0010377781, 0.45...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42960 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prediction\n",
       "id                                                      \n",
       "0      [0.0005371941, 0.28633153, 0.069509454, 0.6436...\n",
       "1      [0.00015652769, 0.20875472, 0.12081046, 0.6702...\n",
       "100    [4.5013534e-05, 0.69266313, 0.006293945, 0.300...\n",
       "1000   [0.21455875, 0.36390203, 0.076663665, 0.34487557]\n",
       "10000  [1.0598835e-06, 0.0009105328, 0.0023490405, 0....\n",
       "...                                                  ...\n",
       "99974  [0.9999947, 7.870432e-07, 1.0888429e-07, 4.364...\n",
       "99983  [0.9999991, 5.2881205e-08, 4.136213e-08, 8.014...\n",
       "99986  [0.99999946, 3.9046526e-08, 8.0310656e-08, 5.0...\n",
       "99987  [0.9999855, 8.9647085e-07, 1.2998103e-06, 1.22...\n",
       "9999   [3.7698182e-06, 0.54326427, 0.0010377781, 0.45...\n",
       "\n",
       "[42960 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68771d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       0.88      0.94      0.91     17341\n",
      "          ru       0.70      0.64      0.67     10379\n",
      "          en       0.93      0.82      0.87     12964\n",
      "       other       0.70      0.74      0.72     15084\n",
      "\n",
      "    accuracy                           0.80     55768\n",
      "   macro avg       0.80      0.79      0.79     55768\n",
      "weighted avg       0.81      0.80      0.80     55768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fda70e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16319,   359,    96,   567],\n",
       "       [  800,  6676,   188,  2715],\n",
       "       [  318,   345, 10665,  1636],\n",
       "       [ 1084,  2197,   577, 11226]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.confusion_matrix(y_true=true_sparse, y_pred=pred_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b29ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
