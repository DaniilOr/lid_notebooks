{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a992fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59f700c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "#test = test.sample(30000, replace=False)\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5999f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some preprocessing to make sure that the path is correct\n",
    "meta.loc[meta[\"locale\"] != \"kz\", \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" +  meta.loc[meta[\"locale\"] != \"kz\"][\"locale\"] + \"/clips/\" + meta.loc[meta[\"locale\"] != \"kz\"][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4130e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"/tf/datasets/augmentedXvector/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc1ccb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"id\"] = meta[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc5f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfbd75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# preprocessing of audios\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "\n",
    "# reading and normalizing data\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "\n",
    "# augmentations using random filtering\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "# significant speed change\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.5, max=1.5, flag=None)\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(S)\n",
    "        mfccs = mfccs[...,1:21]\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "        mfccs_cmvn = cmvn(mfccs)\n",
    "\n",
    "        #S = tfio.audio.freq_mask(S, param=10)\n",
    "        #S = tfio.audio.time_mask(S, param=10)\n",
    "    return dict(x, logmelspec=S, mfccs=mfccs)\n",
    "\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "           #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(32)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(32)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(32)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08b23665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-27 11:18:18.170 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.500, 1.500]\n",
      "2021-06-27 11:18:18.194 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-27 11:18:18.206 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-27 11:18:18.577 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-27 11:18:18.590 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-27 11:18:18.946 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-27 11:18:18.959 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "val_data = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}\n",
    "val_data = val_data['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc223322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-27 11:18:19.549 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.500, 1.500]\n",
      "2021-06-27 11:18:19.572 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-27 11:18:19.584 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-27 11:18:19.944 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-27 11:18:19.957 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-27 11:18:20.307 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-27 11:18:20.320 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "963a65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are all blocks used to build the model. Retrieved from: https://github.com/py-lidbox/lidbox \n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    " \n",
    "   Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Layer,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming spectral features (Batch, Time, Channels), where freq. channels are always last\n",
    "TIME_AXIS = 1\n",
    "STDDEV_SQRT_MIN_CLIP = 1e-10\n",
    "\n",
    "\n",
    "class GlobalMeanStddevPooling1D(Layer):\n",
    "    \"\"\"\n",
    "    Compute arithmetic mean and standard deviation of the inputs along the time steps dimension,\n",
    "    then output the concatenation of the computed stats.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        means = tf.math.reduce_mean(inputs, axis=TIME_AXIS, keepdims=True)\n",
    "        variances = tf.math.reduce_mean(tf.math.square(inputs - means), axis=TIME_AXIS)\n",
    "        means = tf.squeeze(means, TIME_AXIS)\n",
    "        stddevs = tf.math.sqrt(tf.clip_by_value(variances, STDDEV_SQRT_MIN_CLIP, variances.dtype.max))\n",
    "        return tf.concat((means, stddevs), axis=TIME_AXIS)\n",
    "\n",
    "\n",
    "def frame_layer(filters, kernel_size, strides, padding=\"causal\", activation=\"relu\", name=\"frame\"):\n",
    "    return Conv1D(filters, kernel_size, strides, padding=padding, activation=activation, name=name)\n",
    "\n",
    "\n",
    "def segment_layer(units, activation=\"relu\", name=\"segment\"):\n",
    "    return Dense(units, activation=activation, name=name)\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    GaussianNoise,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LSTM,\n",
    "    Multiply,\n",
    "    Reshape,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def frequency_attention(H, d_a=64, d_f=16):\n",
    "    assert not H.shape[2] % d_f, \"amount of frequency channels ({}) must be evenly divisible by the amount of frequency attention bins (d_f={})\".format(H.shape[2], d_f)\n",
    "    # Note, we assume that H.shape = (batch_size, T, d_h), but the paper assumes the timesteps come last\n",
    "    x = Dense(d_a, activation=\"relu\", use_bias=False, name=\"Wf_1\")(H)\n",
    "    F_A = Dense(d_f, activation=\"softmax\", use_bias=False, name=\"Wf_2\")(x)\n",
    "    # Apply frequency attention on d_f bins\n",
    "    F_A = Reshape((F_A.shape[1] or -1, F_A.shape[2], 1), name=\"expand_bin_weight_dim\")(F_A)\n",
    "    H_bins = Reshape((H.shape[1] or -1, d_f, H.shape[2] // d_f), name=\"partition_freq_bins\")(H)\n",
    "    H_bins = Multiply(name=\"freq_attention\")([F_A, H_bins])\n",
    "    # Merge weighted frequency bins\n",
    "    H_weighted = Reshape((H.shape[1] or -1, H.shape[2]), name=\"merge_weighted_bins\")(H_bins)\n",
    "    return H_weighted\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def create(input_shape, num_outputs, output_activation=\"log_softmax\", freq_attention_bins=60):\n",
    "    inputs = Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = frame_layer(512, 5, 1, name=\"frame1\")(inputs)\n",
    "    x = frame_layer(512, 3, 2, name=\"frame2\")(x)\n",
    "    x = frame_layer(512, 3, 3, name=\"frame3\")(x)\n",
    "    x = frame_layer(512, 1, 1, name=\"frame4\")(x)\n",
    "    x = frame_layer(1500, 1, 1, name=\"frame5\")(x)\n",
    "\n",
    "    x = frequency_attention(x, d_f=freq_attention_bins)\n",
    "\n",
    "    x = GlobalMeanStddevPooling1D(name=\"stats_pooling\")(x)\n",
    "\n",
    "    x = segment_layer(512, name=\"segment1\")(x)\n",
    "    x = segment_layer(512, name=\"segment2\")(x)\n",
    "\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=x, name=\"x-vector-frequency-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ebdc8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"x-vector-frequency-attention\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 40)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "frame1 (Conv1D)                 (None, None, 512)    102912      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "frame2 (Conv1D)                 (None, None, 512)    786944      frame1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame3 (Conv1D)                 (None, None, 512)    786944      frame2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame4 (Conv1D)                 (None, None, 512)    262656      frame3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame5 (Conv1D)                 (None, None, 1500)   769500      frame4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_1 (Dense)                    (None, None, 64)     96000       frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_2 (Dense)                    (None, None, 60)     3840        Wf_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "expand_bin_weight_dim (Reshape) (None, None, 60, 1)  0           Wf_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "partition_freq_bins (Reshape)   (None, None, 60, 25) 0           frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "freq_attention (Multiply)       (None, None, 60, 25) 0           expand_bin_weight_dim[0][0]      \n",
      "                                                                 partition_freq_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "merge_weighted_bins (Reshape)   (None, None, 1500)   0           freq_attention[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "stats_pooling (GlobalMeanStddev (None, 3000)         0           merge_weighted_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "segment1 (Dense)                (None, 512)          1536512     stats_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "segment2 (Dense)                (None, 512)          262656      segment1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,607,964\n",
      "Trainable params: 4,607,964\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_freq_bins=40, num_labels=len(np.unique(meta.target))):\n",
    "    m = create(\n",
    "        input_shape=[None, num_freq_bins],\n",
    "        num_outputs=num_labels)\n",
    "    \"\"\"\n",
    "    m.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "    \"\"\"\n",
    "    return m\n",
    "\n",
    "with tf.device(\"GPU\"):\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb0a5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Xvector-based model is used as an encoder. That is why two classifiers will use its outputs\n",
    "model.trainable = False\n",
    "x = model.layers[-1].output\n",
    "x = Dense(128, activation = \"relu\")(x)\n",
    "predictions = Dense(4, activation = \"softmax\")(x)\n",
    "clf1 = Model(inputs = model.input, outputs = predictions, name=\"clf1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad304d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"clf1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 40)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "frame1 (Conv1D)                 (None, None, 512)    102912      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "frame2 (Conv1D)                 (None, None, 512)    786944      frame1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame3 (Conv1D)                 (None, None, 512)    786944      frame2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame4 (Conv1D)                 (None, None, 512)    262656      frame3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame5 (Conv1D)                 (None, None, 1500)   769500      frame4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_1 (Dense)                    (None, None, 64)     96000       frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_2 (Dense)                    (None, None, 60)     3840        Wf_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "expand_bin_weight_dim (Reshape) (None, None, 60, 1)  0           Wf_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "partition_freq_bins (Reshape)   (None, None, 60, 25) 0           frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "freq_attention (Multiply)       (None, None, 60, 25) 0           expand_bin_weight_dim[0][0]      \n",
      "                                                                 partition_freq_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "merge_weighted_bins (Reshape)   (None, None, 1500)   0           freq_attention[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "stats_pooling (GlobalMeanStddev (None, 3000)         0           merge_weighted_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "segment1 (Dense)                (None, 512)          1536512     stats_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "segment2 (Dense)                (None, 512)          262656      segment1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 128)          65664       segment2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 4)            516         dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,674,144\n",
      "Trainable params: 66,180\n",
      "Non-trainable params: 4,607,964\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "157c7edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_24\n",
      "dense_25\n"
     ]
    }
   ],
   "source": [
    "for l in clf1.layers:\n",
    "    if l.trainable:\n",
    "        print(l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e1da554",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d43fbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9b2ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"x-vector-frequency-attention\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 40)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "frame1 (Conv1D)                 (None, None, 512)    102912      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "frame2 (Conv1D)                 (None, None, 512)    786944      frame1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame3 (Conv1D)                 (None, None, 512)    786944      frame2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame4 (Conv1D)                 (None, None, 512)    262656      frame3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame5 (Conv1D)                 (None, None, 1500)   769500      frame4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_1 (Dense)                    (None, None, 64)     96000       frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_2 (Dense)                    (None, None, 60)     3840        Wf_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "expand_bin_weight_dim (Reshape) (None, None, 60, 1)  0           Wf_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "partition_freq_bins (Reshape)   (None, None, 60, 25) 0           frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "freq_attention (Multiply)       (None, None, 60, 25) 0           expand_bin_weight_dim[0][0]      \n",
      "                                                                 partition_freq_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "merge_weighted_bins (Reshape)   (None, None, 1500)   0           freq_attention[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "stats_pooling (GlobalMeanStddev (None, 3000)         0           merge_weighted_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "segment1 (Dense)                (None, 512)          1536512     stats_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "segment2 (Dense)                (None, 512)          262656      segment1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,607,964\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,607,964\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    if l.trainable:\n",
    "        print(l)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f5650678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "x = model.layers[-1].output\n",
    "x = Dense(128, activation = \"relu\")(x)\n",
    "predictions = Dense(4, activation = \"softmax\")(x)\n",
    "clf2 = Model(inputs = model.input, outputs = predictions, name=\"clf2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7ab89fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "439a5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_26\n",
      "dense_27\n"
     ]
    }
   ],
   "source": [
    "clf2.trainable = True\n",
    "model.trainable = False\n",
    "for l in clf2.layers:\n",
    "    if l.trainable:\n",
    "        print(l.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac92c9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f809f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is epoch number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [04:31,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6472115516662598, discrepancy before: 0.056961655616760254, discrepancy loss after: 0.057231366634368896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [08:58,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.13413667678833, discrepancy before: 0.05077614635229111, discrepancy loss after: 0.051046330481767654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [13:25,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.919963836669922, discrepancy before: 0.044328853487968445, discrepancy loss after: 0.042545855045318604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [17:51,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8670859336853027, discrepancy before: 0.04103769361972809, discrepancy loss after: 0.041010163724422455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [22:17,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5211219787597656, discrepancy before: 0.011340435594320297, discrepancy loss after: 0.011625243350863457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [26:42,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.6412782669067383, discrepancy before: 0.034960709512233734, discrepancy loss after: 0.034523457288742065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7000it [31:08,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.044222831726074, discrepancy before: 0.006423259153962135, discrepancy loss after: 0.0063832481391727924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [35:34,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3240962028503418, discrepancy before: 0.004808356985449791, discrepancy loss after: 0.0041030957363545895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9000it [39:59,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1586060523986816, discrepancy before: 0.005310078151524067, discrepancy loss after: 0.005273367743939161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9778it [43:23,  3.76it/s]\n",
      "2991it [01:20, 37.27it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.586567223072052, Train acc: 0.513098955154419. Train loss (clf1): 2.1604323387145996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2991it [01:19, 37.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc (clf2): 0.5862955451011658.\n",
      "Here is an improvement from 0 to 0.5864313840866089\n",
      "Saving results\n",
      "INFO:tensorflow:Assets written to: clf1_3/model/assets\n",
      "2021-06-27 18:38:03.501 I tensorflow: Assets written to: clf1_3/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: clf2_3/model/assets\n",
      "2021-06-27 18:38:05.228 I tensorflow: Assets written to: clf2_3/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: extractor/model/assets\n",
      "2021-06-27 18:38:06.786 I tensorflow: Assets written to: extractor/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is epoch number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "222it [01:03,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4493460655212402, discrepancy before: 0.0064748008735477924, discrepancy loss after: 0.006274324841797352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1222it [05:31,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9953786134719849, discrepancy before: 0.0002067375462502241, discrepancy loss after: 0.00019329233327880502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2222it [09:57,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5613136291503906, discrepancy before: 0.002906474284827709, discrepancy loss after: 0.0026488087605684996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3222it [14:23,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5251617431640625, discrepancy before: 0.0015362376580014825, discrepancy loss after: 0.0013950061984360218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4222it [18:50,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2038086652755737, discrepancy before: 0.008787691593170166, discrepancy loss after: 0.007556402124464512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5222it [23:16,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8245667219161987, discrepancy before: 0.0002649087691679597, discrepancy loss after: 0.00020591109932865947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6222it [27:42,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8843764662742615, discrepancy before: 0.0018520738231018186, discrepancy loss after: 0.0017619709251448512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7222it [32:08,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.863968849182129, discrepancy before: 0.003809521673247218, discrepancy loss after: 0.0038248091004788876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8222it [36:34,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0268434286117554, discrepancy before: 0.0034442367032170296, discrepancy loss after: 0.003364184405654669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9222it [41:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.627358913421631, discrepancy before: 0.005002018064260483, discrepancy loss after: 0.004480269737541676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9776it [43:25,  3.75it/s]\n",
      "2991it [01:19, 37.55it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.6673771739006042, Train acc: 0.6325052976608276. Train loss (clf1): 1.4697000980377197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2991it [01:19, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc (clf2): 0.666907012462616.\n",
      "Here is an improvement from 0.5864313840866089 to 0.6671420931816101\n",
      "Saving results\n",
      "INFO:tensorflow:Assets written to: clf1_3/model/assets\n",
      "2021-06-27 19:24:13.595 I tensorflow: Assets written to: clf1_3/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: clf2_3/model/assets\n",
      "2021-06-27 19:24:15.533 I tensorflow: Assets written to: clf2_3/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: extractor/model/assets\n",
      "2021-06-27 19:24:17.095 I tensorflow: Assets written to: extractor/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is epoch number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "446it [02:02,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9978622198104858, discrepancy before: 2.3635129764443263e-05, discrepancy loss after: 2.488081190676894e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1446it [06:29,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.1384044885635376, discrepancy before: 0.0031542563810944557, discrepancy loss after: 0.003146658418700099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [10:55,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9339596629142761, discrepancy before: 3.12356036147321e-07, discrepancy loss after: 2.8410627805897093e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3446it [15:20,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7237623929977417, discrepancy before: 0.003573259338736534, discrepancy loss after: 0.0035681030713021755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4446it [19:46,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8444103002548218, discrepancy before: 0.002823175862431526, discrepancy loss after: 0.0025392004754394293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4564it [20:18,  3.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-bc7e254f3325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_classification_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdiscrepancy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6465\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_on_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6466\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6468\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training with domain adaptation. It was retrieved from: https://github.com/mil-tokyo/MCD_DA\n",
    "from tqdm import tqdm\n",
    "EPOCHS = 100 \n",
    "dev_iterator = iter(split2ds[\"dev\"].batch(32).repeat(1000))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "best_acc = 0\n",
    "counter = 0\n",
    "MAX_PATIENCE = 5\n",
    "c = 0\n",
    "\n",
    "def discrepancy(out1, out2):\n",
    "    return tf.reduce_mean(tf.abs(out1 - out2))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"It is epoch number: {epoch}\")\n",
    "    avg_loss = 0\n",
    "    for i in tqdm(split2ds[\"train\"].batch(32)):\n",
    "        c += 1\n",
    "        clf1.trainable = True\n",
    "        clf2.trainable = True\n",
    "        model.trainable = True\n",
    "        target_data = next(dev_iterator)\n",
    "        s_specs = i['logmelspec']\n",
    "        t_specs = target_data['logmelspec']\n",
    "        # Train for classification\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred1 = clf1(s_specs)\n",
    "            pred2 = clf2(s_specs)\n",
    "            loss_classification_1 = loss_fn(i['target'], pred1)\n",
    "            loss_classification_2 = loss_fn(i['target'], pred2)\n",
    "            total_classification_loss = loss_classification_1 + loss_classification_2\n",
    "            avg_loss += total_classification_loss\n",
    "            \n",
    "        train_acc_metric.update_state(i['target'], pred1)\n",
    "        grads = tape.gradient(total_classification_loss, [clf1.trainable_weights, clf2.trainable_weights, model.trainable_weights])\n",
    "        optimizer.apply_gradients(zip(grads[0], clf1.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(grads[1], clf2.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(grads[2], model.trainable_weights))\n",
    "        # Train for discrepancy increase\n",
    "        model.trainable = False\n",
    "        with tf.GradientTape() as tape2:\n",
    "            pred1 = clf1(s_specs)\n",
    "            pred2 = clf2(s_specs)\n",
    "            loss_classification_1 = loss_fn(i['target'], pred1)\n",
    "            loss_classification_2 = loss_fn(i['target'], pred2)\n",
    "            total_classification_loss = loss_classification_1 + loss_classification_2\n",
    "            pred1 = clf1(t_specs)\n",
    "            pred2 = clf2(t_specs)\n",
    "            discrepancy_loss = discrepancy(pred1, pred2)\n",
    "            dl1 = discrepancy_loss\n",
    "            loss = total_classification_loss - discrepancy_loss\n",
    "\n",
    "        grads = tape2.gradient(loss, [clf1.trainable_weights, clf2.trainable_weights])\n",
    "        optimizer.apply_gradients(zip(grads[0], clf1.trainable_weights))\n",
    "        optimizer.apply_gradients(zip(grads[1], clf2.trainable_weights))\n",
    "        # Train for discrepancy decrease\n",
    "        clf1.trainable = False\n",
    "        clf2.trainable = False\n",
    "        model.trainable = True\n",
    "        dloss = 0\n",
    "        for k in range(3):\n",
    "            with tf.GradientTape() as tape3:\n",
    "                pred1 = clf1(t_specs)\n",
    "                pred2 = clf2(t_specs)\n",
    "                discrepancy_loss = discrepancy(pred1, pred2)\n",
    "                dloss = discrepancy_loss\n",
    "            grads = tape3.gradient(discrepancy_loss, model.trainable_weights)            \n",
    "            optimizer2.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        if c % 1000 == 0:\n",
    "            tqdm.write(f\"loss: {total_classification_loss}, discrepancy before: {dl1}, discrepancy loss after: {dloss}\")\n",
    "            c = 0\n",
    "    for batch in tqdm(val_data.batch(32)):\n",
    "        val_preds = clf1(batch['logmelspec'])\n",
    "        true_vals = batch['target']  \n",
    "        val_acc_metric.update_state(true_vals, val_preds)\n",
    "    new_acc = 0\n",
    "    val_acc = val_acc_metric.result()\n",
    "    new_acc += val_acc\n",
    "    val_acc_metric.reset_states()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "    print(f\"Val acc: {val_acc}, Train acc: {train_acc}. Train loss (clf1): {avg_loss/9783}\")\n",
    "    for batch in tqdm(val_data.batch(32)):\n",
    "        val_preds = clf2(batch['logmelspec'])\n",
    "        true_vals = batch['target']  \n",
    "        val_acc_metric.update_state(true_vals, val_preds)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "    print(f\"Val acc (clf2): {val_acc}.\")\n",
    "    new_acc += val_acc\n",
    "    if new_acc / 2 > best_acc:\n",
    "        print(f\"Here is an improvement from {best_acc} to {new_acc / 2}\\nSaving results\")\n",
    "        best_acc = new_acc / 2\n",
    "        counter = 0\n",
    "        clf1.save(\"clf1_3/model\")\n",
    "        clf2.save(\"clf2_3/model\")\n",
    "        model.save(\"extractor/model\")\n",
    "    else:\n",
    "        print(\"No improvements\")\n",
    "        counter += 1\n",
    "    if counter >= MAX_PATIENCE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f1f61",
   "metadata": {},
   "source": [
    "## Testing on common voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "05d90a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3af8bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = tf.keras.models.load_model('clf1_2/model')\n",
    "\n",
    "clf2 = tf.keras.models.load_model('clf2_2/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d918adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_dataframe(ids, predictions):\n",
    "    return (pd.DataFrame.from_dict({\"id\": ids, \"prediction\": predictions})\n",
    "            #.set_index(\"id\", drop=True, verify_integrity=True)\n",
    "            #.sort_index()\n",
    "           )\n",
    "\n",
    "def predict_with_model(model, ds, predict_fn=None):\n",
    "    \"\"\"\n",
    "    Map callable model over all batches in ds, predicting values for each element at key 'input'.\n",
    "    \"\"\"\n",
    "    if predict_fn is None:\n",
    "        def predict_fn(x):\n",
    "            with tf.device(\"GPU\"):\n",
    "                return x[\"id\"], model(x[\"input\"], training=False)\n",
    "\n",
    "    ids = []\n",
    "    predictions = []\n",
    "    for id, pred in ds.map(predict_fn, num_parallel_calls=TF_AUTOTUNE).unbatch().as_numpy_iterator():\n",
    "        ids.append(id.decode(\"utf-8\"))\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return predictions_to_dataframe(ids, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3f559c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = predict_with_model(\n",
    "    model=clf2,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    #predict_fn=predict_with_ap_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9abebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = chunk2pred.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "106ccd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424383.mp3</th>\n",
       "      <td>[8.308059e-09, 0.0003079729, 6.093462e-05, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424555.mp3</th>\n",
       "      <td>[2.471661e-07, 0.005254525, 0.00096489495, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424567.mp3</th>\n",
       "      <td>[1.87723e-07, 0.80234873, 5.8355516e-05, 0.197...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424609.mp3</th>\n",
       "      <td>[1.9055422e-05, 0.7478529, 1.27799485e-05, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424636.mp3</th>\n",
       "      <td>[2.5810184e-06, 0.056320507, 0.001680822, 0.94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22242585.mp3</th>\n",
       "      <td>[3.313049e-08, 8.7356224e-05, 0.0008840677, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22242819.mp3</th>\n",
       "      <td>[0.00059783406, 0.08054378, 0.7682407, 0.1506177]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243431.mp3</th>\n",
       "      <td>[1.5969928e-05, 0.0027630564, 0.12403288, 0.87...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243666.mp3</th>\n",
       "      <td>[9.9802e-09, 8.252818e-06, 1.6951846e-05, 0.99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22246300.mp3</th>\n",
       "      <td>[4.939883e-07, 4.9820347e-08, 0.00077171216, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55768 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           prediction\n",
       "id                                                                                                   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [8.308059e-09, 0.0003079729, 6.093462e-05, 0.9...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [2.471661e-07, 0.005254525, 0.00096489495, 0.9...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [1.87723e-07, 0.80234873, 5.8355516e-05, 0.197...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [1.9055422e-05, 0.7478529, 1.27799485e-05, 0.2...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [2.5810184e-06, 0.056320507, 0.001680822, 0.94...\n",
       "...                                                                                               ...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [3.313049e-08, 8.7356224e-05, 0.0008840677, 0....\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [0.00059783406, 0.08054378, 0.7682407, 0.1506177]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [1.5969928e-05, 0.0027630564, 0.12403288, 0.87...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [9.9802e-09, 8.252818e-06, 1.6951846e-05, 0.99...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [4.939883e-07, 4.9820347e-08, 0.00077171216, 0...\n",
       "\n",
       "[55768 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1effc6d5",
   "metadata": {},
   "source": [
    "### Results of clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c6e5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       1.00      1.00      1.00     17341\n",
      "          ru       0.88      0.75      0.81     10379\n",
      "          en       0.87      0.87      0.87     12964\n",
      "       other       0.77      0.84      0.80     15084\n",
      "\n",
      "    accuracy                           0.88     55768\n",
      "   macro avg       0.88      0.86      0.87     55768\n",
      "weighted avg       0.88      0.88      0.88     55768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f244ce",
   "metadata": {},
   "source": [
    "Since the results vary, it is reasonable to conduct the experiment further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e250b",
   "metadata": {},
   "source": [
    "## Testing on vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c19aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = predict_with_model(\n",
    "    model=clf2,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    #predict_fn=predict_with_ap_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "50daf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = chunk2pred.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8bb51883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>locale</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56701</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110475</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>rw</td>\n",
       "      <td>train</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45384</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>en</td>\n",
       "      <td>train</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_dev/BH8c4SbgXss__U__S251---1604.030-1609.420.mp3</th>\n",
       "      <td>/tf/datasets/vox/ru_dev/BH8c4SbgXss__U__S251--...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_dev/--RxvUW3u7M__U__S0---0552.770-0565.180.mp3</th>\n",
       "      <td>/tf/datasets/vox/ru_dev/--RxvUW3u7M__U__S0---0...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_dev/kZ8LKE26cl0__U__S1---0100.470-0114.760.mp3</th>\n",
       "      <td>/tf/datasets/vox/ru_dev/kZ8LKE26cl0__U__S1---0...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_dev/ya9uyy12vvM__U__S208---1384.770-1393.130.mp3</th>\n",
       "      <td>/tf/datasets/vox/ru_dev/ya9uyy12vvM__U__S208--...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_dev/lUhjPr7Lcu8__U__S100---0687.990-0692.580.mp3</th>\n",
       "      <td>/tf/datasets/vox/ru_dev/lUhjPr7Lcu8__U__S100--...</td>\n",
       "      <td>ru</td>\n",
       "      <td>dev</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162610 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 path  \\\n",
       "id                                                                                                      \n",
       "1486                                                /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "56701                                               /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "3364                                                /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "110475                                              /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "45384                                               /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "...                                                                                               ...   \n",
       "/tf/datasets/vox/ru_dev/BH8c4SbgXss__U__S251---...  /tf/datasets/vox/ru_dev/BH8c4SbgXss__U__S251--...   \n",
       "/tf/datasets/vox/ru_dev/--RxvUW3u7M__U__S0---05...  /tf/datasets/vox/ru_dev/--RxvUW3u7M__U__S0---0...   \n",
       "/tf/datasets/vox/ru_dev/kZ8LKE26cl0__U__S1---01...  /tf/datasets/vox/ru_dev/kZ8LKE26cl0__U__S1---0...   \n",
       "/tf/datasets/vox/ru_dev/ya9uyy12vvM__U__S208---...  /tf/datasets/vox/ru_dev/ya9uyy12vvM__U__S208--...   \n",
       "/tf/datasets/vox/ru_dev/lUhjPr7Lcu8__U__S100---...  /tf/datasets/vox/ru_dev/lUhjPr7Lcu8__U__S100--...   \n",
       "\n",
       "                                                   locale  split  target  \n",
       "id                                                                        \n",
       "1486                                                   ru  train     1.0  \n",
       "56701                                                  kz  train     0.0  \n",
       "3364                                                   ru  train     1.0  \n",
       "110475                                                 rw  train     3.0  \n",
       "45384                                                  en  train     2.0  \n",
       "...                                                   ...    ...     ...  \n",
       "/tf/datasets/vox/ru_dev/BH8c4SbgXss__U__S251---...     ru    dev     1.0  \n",
       "/tf/datasets/vox/ru_dev/--RxvUW3u7M__U__S0---05...     ru    dev     1.0  \n",
       "/tf/datasets/vox/ru_dev/kZ8LKE26cl0__U__S1---01...     ru    dev     1.0  \n",
       "/tf/datasets/vox/ru_dev/ya9uyy12vvM__U__S208---...     ru    dev     1.0  \n",
       "/tf/datasets/vox/ru_dev/lUhjPr7Lcu8__U__S100---...     ru    dev     1.0  \n",
       "\n",
       "[162610 rows x 4 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f116f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3</th>\n",
       "      <td>[0.82074064, 0.036507558, 0.05600509, 0.086746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0743.730-0757.100.mp3</th>\n",
       "      <td>[0.7609731, 0.050528564, 0.07821112, 0.11028719]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100---0650.920-0661.560.mp3</th>\n",
       "      <td>[0.99982405, 1.17218315e-05, 0.00012380008, 4....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100---0692.790-0704.510.mp3</th>\n",
       "      <td>[0.99906653, 9.269332e-05, 0.0005545609, 0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100---0705.010-0711.610.mp3</th>\n",
       "      <td>[0.9915779, 0.0011870286, 0.0039456706, 0.0032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0219.180-0230.690.mp3</th>\n",
       "      <td>[0.9704744, 0.0048984527, 0.01188823, 0.012738...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0230.690-0247.370.mp3</th>\n",
       "      <td>[0.9708305, 0.005451752, 0.0102613745, 0.01345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0247.370-0257.750.mp3</th>\n",
       "      <td>[0.9644639, 0.006060426, 0.01391795, 0.015557733]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0277.000-0287.980.mp3</th>\n",
       "      <td>[0.80379754, 0.04677399, 0.054464877, 0.09496365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---0301.200-0306.760.mp3</th>\n",
       "      <td>[0.56486857, 0.08920277, 0.1341035, 0.21182513]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36053 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           prediction\n",
       "id                                                                                                   \n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...  [0.82074064, 0.036507558, 0.05600509, 0.086746...\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...   [0.7609731, 0.050528564, 0.07821112, 0.11028719]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100--...  [0.99982405, 1.17218315e-05, 0.00012380008, 4....\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100--...  [0.99906653, 9.269332e-05, 0.0005545609, 0.000...\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100--...  [0.9915779, 0.0011870286, 0.0039456706, 0.0032...\n",
       "...                                                                                               ...\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [0.9704744, 0.0048984527, 0.01188823, 0.012738...\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [0.9708305, 0.005451752, 0.0102613745, 0.01345...\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [0.9644639, 0.006060426, 0.01391795, 0.015557733]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [0.80379754, 0.04677399, 0.054464877, 0.09496365]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---...    [0.56486857, 0.08920277, 0.1341035, 0.21182513]\n",
       "\n",
       "[36053 rows x 1 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e4856cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       0.39      0.99      0.55     13946\n",
      "          ru       0.00      0.00      0.00     12107\n",
      "          en       0.47      0.00      0.00     10000\n",
      "       other       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.38     36053\n",
      "   macro avg       0.21      0.25      0.14     36053\n",
      "weighted avg       0.28      0.38      0.22     36053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
