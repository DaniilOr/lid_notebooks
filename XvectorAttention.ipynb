{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36b18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2950703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "#test = test.sample(30000, replace=False)\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0cdd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-04 13:15:07.904 I numexpr.utils: Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2021-06-04 13:15:07.906 I numexpr.utils: Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-06-04 13:15:07.906 I numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "meta.loc[meta[\"locale\"] != \"kz\", \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" +  meta.loc[meta[\"locale\"] != \"kz\"][\"locale\"] + \"/clips/\" + meta.loc[meta[\"locale\"] != \"kz\"][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n",
    "workdir = \"/tf/datasets/xvectorAttention\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdcbe569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-04 13:15:11.474 W tensorflow: From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-04 13:15:11.646 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.900, 1.100]\n",
      "2021-06-04 13:15:11.895 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-04 13:15:11.995 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-04 13:15:13.961 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-04 13:15:13.978 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-04 13:15:14.364 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-04 13:15:14.380 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.9, max=1.1, flag=None)\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "           #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9c8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Layer,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def frame_layer(filters, kernel_size, strides, padding=\"causal\", activation=\"relu\", name=\"frame\"):\n",
    "    return Conv1D(filters, kernel_size, strides, padding=padding, activation=activation, name=name)\n",
    "\n",
    "\n",
    "def segment_layer(units, activation=\"relu\", name=\"segment\"):\n",
    "    return Dense(units, activation=activation, name=name)\n",
    "class GlobalMeanStddevPooling1D(Layer):\n",
    "    \"\"\"\n",
    "    Compute arithmetic mean and standard deviation of the inputs along the time steps dimension,\n",
    "    then output the concatenation of the computed stats.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        means = tf.math.reduce_mean(inputs, axis=TIME_AXIS, keepdims=True)\n",
    "        variances = tf.math.reduce_mean(tf.math.square(inputs - means), axis=TIME_AXIS)\n",
    "        means = tf.squeeze(means, TIME_AXIS)\n",
    "        stddevs = tf.math.sqrt(tf.clip_by_value(variances, STDDEV_SQRT_MIN_CLIP, variances.dtype.max))\n",
    "        return tf.concat((means, stddevs), axis=TIME_AXIS)\n",
    "\n",
    "def as_embedding_extractor(m):\n",
    "    l = m.get_layer(name=\"segment1\")\n",
    "    l.activation = None\n",
    "    return Model(inputs=m.inputs, outputs=l.output)\n",
    "\n",
    "def frequency_attention(H, d_a=64, d_f=16):\n",
    "    assert not H.shape[2] % d_f, \"amount of frequency channels ({}) must be evenly divisible by the amount of frequency attention bins (d_f={})\".format(H.shape[2], d_f)\n",
    "    # Note, we assume that H.shape = (batch_size, T, d_h), but the paper assumes the timesteps come last\n",
    "    x = Dense(d_a, activation=\"relu\", use_bias=False, name=\"Wf_1\")(H)\n",
    "    F_A = Dense(d_f, activation=\"softmax\", use_bias=False, name=\"Wf_2\")(x)\n",
    "    # Apply frequency attention on d_f bins\n",
    "    F_A = Reshape((F_A.shape[1] or -1, F_A.shape[2], 1), name=\"expand_bin_weight_dim\")(F_A)\n",
    "    H_bins = Reshape((H.shape[1] or -1, d_f, H.shape[2] // d_f), name=\"partition_freq_bins\")(H)\n",
    "    H_bins = Multiply(name=\"freq_attention\")([F_A, H_bins])\n",
    "    # Merge weighted frequency bins\n",
    "    H_weighted = Reshape((H.shape[1] or -1, H.shape[2]), name=\"merge_weighted_bins\")(H_bins)\n",
    "    return H_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc09fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    GaussianNoise,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LSTM,\n",
    "    Multiply,\n",
    "    Reshape,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming spectral features (Batch, Time, Channels), where freq. channels are always last\n",
    "TIME_AXIS = 1\n",
    "STDDEV_SQRT_MIN_CLIP = 1e-10\n",
    "\n",
    "def create(input_shape, num_outputs, output_activation=\"log_softmax\", freq_attention_bins=60):\n",
    "    inputs = Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = frame_layer(512, 5, 1, name=\"frame1\")(inputs)\n",
    "    x = frame_layer(512, 3, 2, name=\"frame2\")(x)\n",
    "    x = frame_layer(512, 3, 3, name=\"frame3\")(x)\n",
    "    x = frame_layer(512, 1, 1, name=\"frame4\")(x)\n",
    "    x = frame_layer(1500, 1, 1, name=\"frame5\")(x)\n",
    "\n",
    "    x = frequency_attention(x, d_f=freq_attention_bins)\n",
    "\n",
    "    x = GlobalMeanStddevPooling1D(name=\"stats_pooling\")(x)\n",
    "\n",
    "    x = segment_layer(512, name=\"segment1\")(x)\n",
    "    x = segment_layer(512, name=\"segment2\")(x)\n",
    "\n",
    "    outputs = Dense(num_outputs, name=\"output\", activation=None)(x)\n",
    "    if output_activation:\n",
    "        outputs = Activation(getattr(tf.nn, output_activation), name=str(output_activation))(outputs)\n",
    "    return Model(inputs=inputs, outputs=outputs, name=\"x-vector-frequency-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a0b37f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"x-vector-frequency-attention\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 40)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "frame1 (Conv1D)                 (None, None, 512)    102912      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "frame2 (Conv1D)                 (None, None, 512)    786944      frame1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame3 (Conv1D)                 (None, None, 512)    786944      frame2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame4 (Conv1D)                 (None, None, 512)    262656      frame3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame5 (Conv1D)                 (None, None, 1500)   769500      frame4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_1 (Dense)                    (None, None, 64)     96000       frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_2 (Dense)                    (None, None, 60)     3840        Wf_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "expand_bin_weight_dim (Reshape) (None, None, 60, 1)  0           Wf_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "partition_freq_bins (Reshape)   (None, None, 60, 25) 0           frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "freq_attention (Multiply)       (None, None, 60, 25) 0           expand_bin_weight_dim[0][0]      \n",
      "                                                                 partition_freq_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "merge_weighted_bins (Reshape)   (None, None, 1500)   0           freq_attention[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "stats_pooling (GlobalMeanStddev (None, 3000)         0           merge_weighted_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "segment1 (Dense)                (None, 512)          1536512     stats_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "segment2 (Dense)                (None, 512)          262656      segment1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 4)            2052        segment2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "log_softmax (Activation)        (None, 4)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,610,016\n",
      "Trainable params: 4,610,016\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_model(num_freq_bins=40, num_labels=len(np.unique(meta.target))):\n",
    "    m = create(\n",
    "        input_shape=[None, num_freq_bins],\n",
    "        num_outputs=num_labels)\n",
    "    m.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "    return m\n",
    "\n",
    "with tf.device(\"GPU\"):\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "   \n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing datasets\n",
      "start training\n",
      "Epoch 1/100\n",
      "1301/2576 [==============>...............] - ETA: 2:51 - loss: 0.7291 - sparse_categorical_accuracy: 0.6099"
     ]
    }
   ],
   "source": [
    "print(\"preparing datasets\")\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input)\n",
    "dev_ds = split2ds[\"dev\"].map(as_model_input)\n",
    "\n",
    " \n",
    "print(\"start training\")    \n",
    "with tf.device(\"GPU\"):\n",
    "    history = model.fit(\n",
    "        train_ds.batch(32).repeat(100),\n",
    "        steps_per_epoch=2576,\n",
    "        validation_data=dev_ds.batch(32).repeat(100),\n",
    "        validation_steps=961,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        epochs=100)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model, \"xvectorAttention.h5\", overwrite=True, include_optimizer=True, save_format=None,\n",
    "    signatures=None, options=None, save_traces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a5276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa66712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
