{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79becaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9eeb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "#test = test.sample(30000, replace=False)\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7185b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[meta[\"locale\"] != \"kz\", \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" +  meta.loc[meta[\"locale\"] != \"kz\"][\"locale\"] + \"/clips/\" + meta.loc[meta[\"locale\"] != \"kz\"][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n",
    "workdir = \"/tf/datasets/clstm/v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4dfe8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-05 10:17:51.913 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.900, 1.100]\n",
      "2021-06-05 10:17:51.937 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-05 10:17:51.949 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-05 10:17:52.304 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-05 10:17:52.317 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-05 10:17:52.656 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-05 10:17:52.670 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.9, max=1.1, flag=None)\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "           #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49810a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are all blocks used to build the model. Retrieved from: https://github.com/py-lidbox/lidbox \n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Layer,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def frame_layer(filters, kernel_size, strides, padding=\"causal\", activation=\"relu\", name=\"frame\"):\n",
    "    return Conv1D(filters, kernel_size, strides, padding=padding, activation=activation, name=name)\n",
    "\n",
    "\n",
    "def segment_layer(units, activation=\"relu\", name=\"segment\"):\n",
    "    return Dense(units, activation=activation, name=name)\n",
    "class GlobalMeanStddevPooling1D(Layer):\n",
    "    \"\"\"\n",
    "    Compute arithmetic mean and standard deviation of the inputs along the time steps dimension,\n",
    "    then output the concatenation of the computed stats.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        means = tf.math.reduce_mean(inputs, axis=TIME_AXIS, keepdims=True)\n",
    "        variances = tf.math.reduce_mean(tf.math.square(inputs - means), axis=TIME_AXIS)\n",
    "        means = tf.squeeze(means, TIME_AXIS)\n",
    "        stddevs = tf.math.sqrt(tf.clip_by_value(variances, STDDEV_SQRT_MIN_CLIP, variances.dtype.max))\n",
    "        return tf.concat((means, stddevs), axis=TIME_AXIS)\n",
    "\n",
    "def as_embedding_extractor(m):\n",
    "    l = m.get_layer(name=\"segment1\")\n",
    "    l.activation = None\n",
    "    return Model(inputs=m.inputs, outputs=l.output)\n",
    "\n",
    "def frequency_attention(H, d_a=64, d_f=16):\n",
    "    assert not H.shape[2] % d_f, \"amount of frequency channels ({}) must be evenly divisible by the amount of frequency attention bins (d_f={})\".format(H.shape[2], d_f)\n",
    "    # Note, we assume that H.shape = (batch_size, T, d_h), but the paper assumes the timesteps come last\n",
    "    x = Dense(d_a, activation=\"relu\", use_bias=False, name=\"Wf_1\")(H)\n",
    "    F_A = Dense(d_f, activation=\"softmax\", use_bias=False, name=\"Wf_2\")(x)\n",
    "    # Apply frequency attention on d_f bins\n",
    "    F_A = Reshape((F_A.shape[1] or -1, F_A.shape[2], 1), name=\"expand_bin_weight_dim\")(F_A)\n",
    "    H_bins = Reshape((H.shape[1] or -1, d_f, H.shape[2] // d_f), name=\"partition_freq_bins\")(H)\n",
    "    H_bins = Multiply(name=\"freq_attention\")([F_A, H_bins])\n",
    "    # Merge weighted frequency bins\n",
    "    H_weighted = Reshape((H.shape[1] or -1, H.shape[2]), name=\"merge_weighted_bins\")(H_bins)\n",
    "    return H_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c23bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    GaussianNoise,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LSTM,\n",
    "    Multiply,\n",
    "    Reshape,\n",
    ")\n",
    "# Assuming spectral features (Batch, Time, Channels), where freq. channels are always last\n",
    "TIME_AXIS = 1\n",
    "STDDEV_SQRT_MIN_CLIP = 1e-10\n",
    "def frequency_attention(H, d_a=64, d_f=16):\n",
    "    assert not H.shape[2] % d_f, \"amount of frequency channels ({}) must be evenly divisible by the amount of frequency attention bins (d_f={})\".format(H.shape[2], d_f)\n",
    "    # Note, we assume that H.shape = (batch_size, T, d_h), but the paper assumes the timesteps come last\n",
    "    x = Dense(d_a, activation=\"relu\", use_bias=False, name=\"Wf_1\")(H)\n",
    "    F_A = Dense(d_f, activation=\"softmax\", use_bias=False, name=\"Wf_2\")(x)\n",
    "    # Apply frequency attention on d_f bins\n",
    "    F_A = Reshape((F_A.shape[1] or -1, F_A.shape[2], 1), name=\"expand_bin_weight_dim\")(F_A)\n",
    "    H_bins = Reshape((H.shape[1] or -1, d_f, H.shape[2] // d_f), name=\"partition_freq_bins\")(H)\n",
    "    H_bins = Multiply(name=\"freq_attention\")([F_A, H_bins])\n",
    "    # Merge weighted frequency bins\n",
    "    H_weighted = Reshape((H.shape[1] or -1, H.shape[2]), name=\"merge_weighted_bins\")(H_bins)\n",
    "    return H_weighted\n",
    "\n",
    "\n",
    "def create(input_shape, num_outputs, output_activation=\"log_softmax\", use_attention=False, use_conv2d=False, use_lstm=False):\n",
    "    inputs = Input(shape=input_shape, name=\"input\")\n",
    "    x = inputs\n",
    "    x = GaussianNoise(stddev=0.01, name=\"input_noise\")(x)\n",
    "    x = Dropout(rate=0.4, noise_shape=(None, 1, input_shape[1]), name=\"channel_dropout\")(x)\n",
    "\n",
    "    if use_conv2d:\n",
    "        x = Reshape((input_shape[0] or -1, input_shape[1], 1), name=\"reshape_to_image\")(x)\n",
    "        x = Conv2D(128, (3, 9), (1, 6), activation=None, padding=\"same\", name=\"conv2d_1\")(x)\n",
    "        x = BatchNormalization(name=\"conv2d_1_bn\")(x)\n",
    "        x = Activation(\"relu\", name=\"conv2d_1_relu\")(x)\n",
    "        x = Conv2D(256, (3, 9), (1, 6), activation=None, padding=\"same\", name=\"conv2d_2\")(x)\n",
    "        x = BatchNormalization(name=\"conv2d_2_bn\")(x)\n",
    "        x = Activation(\"relu\", name=\"conv2d_2_relu\")(x)\n",
    "        # x = Reshape((x.shape[1] or -1, x.shape[2] * x.shape[3]), name=\"flatten_image_channels\")(x)\n",
    "        x = tf.math.reduce_max(x, axis=2, name=\"maxpool_image_channels\")\n",
    "\n",
    "    x = frame_layer(512, 5, 1, name=\"frame1\")(x)\n",
    "    x = frame_layer(512, 3, 2, name=\"frame2\")(x)\n",
    "    x = frame_layer(512, 3, 3, name=\"frame3\")(x)\n",
    "    if use_lstm:\n",
    "        x = LSTM(512, name=\"lstm\", return_sequences=True)(x)\n",
    "\n",
    "    x = frame_layer(512, 1, 1, name=\"frame4\")(x)\n",
    "    x = frame_layer(1500, 1, 1, name=\"frame5\")(x)\n",
    "    if use_attention:\n",
    "        x = frequency_attention(x, d_f=60)\n",
    "\n",
    "    x = GlobalMeanStddevPooling1D(name=\"stats_pooling\")(x)\n",
    "\n",
    "    x = segment_layer(512, name=\"segment1\")(x)\n",
    "    x = segment_layer(512, name=\"segment2\")(x)\n",
    "    outputs = Dense(num_outputs, name=\"output\", activation=None)(x)\n",
    "\n",
    "    if output_activation:\n",
    "        outputs = Activation(getattr(tf.nn, output_activation), name=str(output_activation))(outputs)\n",
    "    return Model(inputs=inputs, outputs=outputs, name=\"CLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0265e84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CLSTM\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 40)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_noise (GaussianNoise)     (None, None, 40)     0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "channel_dropout (Dropout)       (None, None, 40)     0           input_noise[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_to_image (Reshape)      (None, None, 40, 1)  0           channel_dropout[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, 7, 128) 3584        reshape_to_image[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1_bn (BatchNormalization (None, None, 7, 128) 512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1_relu (Activation)      (None, None, 7, 128) 0           conv2d_1_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, 2, 256) 884992      conv2d_1_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2_bn (BatchNormalization (None, None, 2, 256) 1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2_relu (Activation)      (None, None, 2, 256) 0           conv2d_2_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_1 (TFOpLambd (None, None, 256)    0           conv2d_2_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "frame1 (Conv1D)                 (None, None, 512)    655872      tf.math.reduce_max_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "frame2 (Conv1D)                 (None, None, 512)    786944      frame1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame3 (Conv1D)                 (None, None, 512)    786944      frame2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, None, 512)    2099200     frame3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "frame4 (Conv1D)                 (None, None, 512)    262656      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "frame5 (Conv1D)                 (None, None, 1500)   769500      frame4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_1 (Dense)                    (None, None, 64)     96000       frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Wf_2 (Dense)                    (None, None, 60)     3840        Wf_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "expand_bin_weight_dim (Reshape) (None, None, 60, 1)  0           Wf_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "partition_freq_bins (Reshape)   (None, None, 60, 25) 0           frame5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "freq_attention (Multiply)       (None, None, 60, 25) 0           expand_bin_weight_dim[0][0]      \n",
      "                                                                 partition_freq_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "merge_weighted_bins (Reshape)   (None, None, 1500)   0           freq_attention[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "stats_pooling (GlobalMeanStddev (None, 3000)         0           merge_weighted_bins[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "segment1 (Dense)                (None, 512)          1536512     stats_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "segment2 (Dense)                (None, 512)          262656      segment1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 4)            2052        segment2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "log_softmax (Activation)        (None, 4)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,152,288\n",
      "Trainable params: 8,151,520\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_freq_bins=40, num_labels=len(np.unique(meta.target))):\n",
    "    m = create(\n",
    "        input_shape=[None, num_freq_bins],\n",
    "        use_attention=True, use_conv2d=True, use_lstm=True,\n",
    "        num_outputs=num_labels)\n",
    "    m.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "    return m\n",
    "\n",
    "with tf.device(\"GPU\"):\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "   \n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9880e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de7a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing datasets\n",
      "start training\n",
      "Epoch 1/100\n",
      "2576/2576 [==============================] - 728s 280ms/step - loss: 0.7686 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.7740 - val_sparse_categorical_accuracy: 0.6188\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77399, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 2/100\n",
      "2576/2576 [==============================] - 571s 222ms/step - loss: 0.6549 - sparse_categorical_accuracy: 0.6791 - val_loss: 0.6202 - val_sparse_categorical_accuracy: 0.7242\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77399 to 0.62019, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 3/100\n",
      "2576/2576 [==============================] - 732s 284ms/step - loss: 0.5467 - sparse_categorical_accuracy: 0.7560 - val_loss: 0.5936 - val_sparse_categorical_accuracy: 0.7549\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.62019 to 0.59355, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 4/100\n",
      "2576/2576 [==============================] - 729s 283ms/step - loss: 0.4533 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.4809 - val_sparse_categorical_accuracy: 0.7959\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59355 to 0.48086, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 5/100\n",
      "2576/2576 [==============================] - 708s 275ms/step - loss: 0.4085 - sparse_categorical_accuracy: 0.8255 - val_loss: 0.4275 - val_sparse_categorical_accuracy: 0.8240\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48086 to 0.42747, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 6/100\n",
      "2576/2576 [==============================] - 559s 217ms/step - loss: 0.3696 - sparse_categorical_accuracy: 0.8450 - val_loss: 0.3922 - val_sparse_categorical_accuracy: 0.8356\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42747 to 0.39219, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 7/100\n",
      "2576/2576 [==============================] - 577s 224ms/step - loss: 0.3390 - sparse_categorical_accuracy: 0.8609 - val_loss: 0.4383 - val_sparse_categorical_accuracy: 0.8124\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.39219\n",
      "Epoch 8/100\n",
      "2576/2576 [==============================] - 712s 276ms/step - loss: 0.3213 - sparse_categorical_accuracy: 0.8662 - val_loss: 0.3093 - val_sparse_categorical_accuracy: 0.8734\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39219 to 0.30933, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 9/100\n",
      "2576/2576 [==============================] - 699s 271ms/step - loss: 0.3021 - sparse_categorical_accuracy: 0.8776 - val_loss: 0.3202 - val_sparse_categorical_accuracy: 0.8704\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30933\n",
      "Epoch 10/100\n",
      "2576/2576 [==============================] - 533s 207ms/step - loss: 0.2769 - sparse_categorical_accuracy: 0.8867 - val_loss: 0.2871 - val_sparse_categorical_accuracy: 0.8856\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30933 to 0.28709, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 11/100\n",
      "2576/2576 [==============================] - 582s 226ms/step - loss: 0.2682 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.2934 - val_sparse_categorical_accuracy: 0.8809\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.28709\n",
      "Epoch 12/100\n",
      "2576/2576 [==============================] - 692s 269ms/step - loss: 0.2591 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.2813 - val_sparse_categorical_accuracy: 0.8894\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28709 to 0.28131, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 13/100\n",
      "2576/2576 [==============================] - 645s 250ms/step - loss: 0.2407 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.2637 - val_sparse_categorical_accuracy: 0.8950\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.28131 to 0.26374, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 14/100\n",
      "2576/2576 [==============================] - 601s 233ms/step - loss: 0.2334 - sparse_categorical_accuracy: 0.9065 - val_loss: 0.2554 - val_sparse_categorical_accuracy: 0.8992\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.26374 to 0.25542, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 15/100\n",
      "2576/2576 [==============================] - 553s 215ms/step - loss: 0.2302 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.3246 - val_sparse_categorical_accuracy: 0.8677\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.25542\n",
      "Epoch 16/100\n",
      "2576/2576 [==============================] - 675s 262ms/step - loss: 0.2162 - sparse_categorical_accuracy: 0.9153 - val_loss: 0.2424 - val_sparse_categorical_accuracy: 0.9067\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.25542 to 0.24241, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 17/100\n",
      "2576/2576 [==============================] - 665s 258ms/step - loss: 0.2142 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.2350 - val_sparse_categorical_accuracy: 0.9094\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.24241 to 0.23503, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 18/100\n",
      "2576/2576 [==============================] - 577s 224ms/step - loss: 0.2123 - sparse_categorical_accuracy: 0.9156 - val_loss: 0.2383 - val_sparse_categorical_accuracy: 0.9048\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.23503\n",
      "Epoch 19/100\n",
      "2576/2576 [==============================] - 555s 215ms/step - loss: 0.1976 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.2744 - val_sparse_categorical_accuracy: 0.8833\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.23503\n",
      "Epoch 20/100\n",
      "2576/2576 [==============================] - 584s 227ms/step - loss: 0.1908 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.2036 - val_sparse_categorical_accuracy: 0.9212\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.23503 to 0.20362, saving model to /tf/datasets/clstm/cache/model/CLSTM\n",
      "Epoch 21/100\n",
      "2398/2576 [==========================>...] - ETA: 32s - loss: 0.1942 - sparse_categorical_accuracy: 0.9235"
     ]
    }
   ],
   "source": [
    "print(\"preparing datasets\")\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input)\n",
    "dev_ds = split2ds[\"dev\"].map(as_model_input)\n",
    "\n",
    " \n",
    "print(\"start training\")    \n",
    "with tf.device(\"GPU\"):\n",
    "    history = model.fit(\n",
    "        train_ds.batch(32).repeat(100),\n",
    "        steps_per_epoch=2576,\n",
    "        validation_data=dev_ds.batch(32).repeat(100),\n",
    "        validation_steps=961,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        epochs=100)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model, \"clstm.h5\", overwrite=True, include_optimizer=True, save_format=None,\n",
    "    signatures=None, options=None, save_traces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822888ae",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25c93309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lidbox.util import predict_with_model, classification_report\n",
    "from lidbox.visualize import draw_confusion_matrix\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    model = create_model()\n",
    "    print(os.path.join(cachedir, \"model\", model.name))\n",
    "    model.load_weights(os.path.join(cachedir, \"model\", model.name))\n",
    "    return model\n",
    "\n",
    "\n",
    "def display_classification_report(report):\n",
    "    for m in (\"avg_detection_cost\", \"avg_equal_error_rate\", \"accuracy\"):\n",
    "        print(\"{}: {:.3f}\".format(m, report[m]))\n",
    "\n",
    "    lang_metrics = pd.DataFrame.from_dict(\n",
    "        {k: v for k, v in report.items() if k in lang2target})\n",
    "    lang_metrics[\"mean\"] = lang_metrics.mean(axis=1)\n",
    "    display(lang_metrics.T)\n",
    "\n",
    "    fig, ax = draw_confusion_matrix(report[\"confusion_matrix\"], lang2target)\n",
    "\n",
    "\n",
    "def predict_with_ap_loss(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        # Generate language vector for input spectra\n",
    "        language_vector = model(x[\"input\"], training=False)\n",
    "        print(language_vector)\n",
    "        # Predict languages by computing distances to reference directions\n",
    "        return x[\"id\"], model.loss.predict(language_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfe5a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_dataframe(ids, predictions):\n",
    "    return (pd.DataFrame.from_dict({\"id\": ids, \"prediction\": predictions})\n",
    "            #.set_index(\"id\", drop=True, verify_integrity=True)\n",
    "            #.sort_index()\n",
    "           )\n",
    "\n",
    "def predict_with_model(model, ds, predict_fn=None):\n",
    "    \"\"\"\n",
    "    Map callable model over all batches in ds, predicting values for each element at key 'input'.\n",
    "    \"\"\"\n",
    "    if predict_fn is None:\n",
    "        def predict_fn(x):\n",
    "            with tf.device(\"GPU\"):\n",
    "                return x[\"id\"], model(x[\"input\"], training=False)\n",
    "\n",
    "    ids = []\n",
    "    predictions = []\n",
    "    for id, pred in ds.map(predict_fn, num_parallel_calls=TF_AUTOTUNE).unbatch().as_numpy_iterator():\n",
    "        ids.append(id.decode(\"utf-8\"))\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return predictions_to_dataframe(ids, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9869881",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = predict_with_model(\n",
    "    model=model,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    #predict_fn=predict_with_ap_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37d7efa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71684-000001</td>\n",
       "      <td>[-13.30624, -9.133918, -0.004106542, -5.524336]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71684-000002</td>\n",
       "      <td>[-15.154707, -10.096367, -0.0044609793, -5.424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71684-000003</td>\n",
       "      <td>[-12.682432, -9.2850895, -0.027603451, -3.6071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71684-000004</td>\n",
       "      <td>[-14.768769, -10.79163, -0.021612717, -3.8462431]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88574-000001</td>\n",
       "      <td>[-7.510157e-06, -14.516629, -14.7174835, -11.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175433</th>\n",
       "      <td>259-000003</td>\n",
       "      <td>[-13.283055, -0.095541224, -10.982233, -2.3957...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175434</th>\n",
       "      <td>12245-000001</td>\n",
       "      <td>[-14.400289, -10.104791, -4.658653, -0.0095663]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175435</th>\n",
       "      <td>12245-000002</td>\n",
       "      <td>[-12.039898, -8.260168, -4.4474573, -0.012045147]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175436</th>\n",
       "      <td>12245-000003</td>\n",
       "      <td>[-11.383154, -7.04705, -5.0004334, -0.0076455115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175437</th>\n",
       "      <td>12245-000004</td>\n",
       "      <td>[-10.885882, -6.570616, -4.353207, -0.014388264]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175438 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                         prediction\n",
       "0       71684-000001    [-13.30624, -9.133918, -0.004106542, -5.524336]\n",
       "1       71684-000002  [-15.154707, -10.096367, -0.0044609793, -5.424...\n",
       "2       71684-000003  [-12.682432, -9.2850895, -0.027603451, -3.6071...\n",
       "3       71684-000004  [-14.768769, -10.79163, -0.021612717, -3.8462431]\n",
       "4       88574-000001  [-7.510157e-06, -14.516629, -14.7174835, -11.9...\n",
       "...              ...                                                ...\n",
       "175433    259-000003  [-13.283055, -0.095541224, -10.982233, -2.3957...\n",
       "175434  12245-000001    [-14.400289, -10.104791, -4.658653, -0.0095663]\n",
       "175435  12245-000002  [-12.039898, -8.260168, -4.4474573, -0.012045147]\n",
       "175436  12245-000003  [-11.383154, -7.04705, -5.0004334, -0.0076455115]\n",
       "175437  12245-000004   [-10.885882, -6.570616, -4.353207, -0.014388264]\n",
       "\n",
       "[175438 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "416667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = chunk2pred.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a973a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3194d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-8.872559, -4.3141966, -4.0708833, -0.23141177]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-9.209547, -5.4918914, -3.7692785, -0.49501362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[-10.391207, -1.8143113, -4.7090473, -1.395344]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[-7.1282005, -6.5735254, -5.7789445, -2.4015903]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>[-17.935022, -13.640357, -5.5472155, -0.003907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>[-0.0030593998, -9.06852, -9.783803, -7.0084653]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>[0.0, -24.503967, -25.179514, -19.886988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>[-1.788139e-07, -23.584446, -23.778034, -17.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>[-1.570562e-05, -12.4975395, -16.794548, -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[-9.295773, -1.3701601, -6.1683874, -0.35691458]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42960 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prediction\n",
       "id                                                      \n",
       "0       [-8.872559, -4.3141966, -4.0708833, -0.23141177]\n",
       "1       [-9.209547, -5.4918914, -3.7692785, -0.49501362]\n",
       "100      [-10.391207, -1.8143113, -4.7090473, -1.395344]\n",
       "1000    [-7.1282005, -6.5735254, -5.7789445, -2.4015903]\n",
       "10000  [-17.935022, -13.640357, -5.5472155, -0.003907...\n",
       "...                                                  ...\n",
       "99974   [-0.0030593998, -9.06852, -9.783803, -7.0084653]\n",
       "99983          [0.0, -24.503967, -25.179514, -19.886988]\n",
       "99986  [-1.788139e-07, -23.584446, -23.778034, -17.07...\n",
       "99987  [-1.570562e-05, -12.4975395, -16.794548, -11.8...\n",
       "9999    [-9.295773, -1.3701601, -6.1683874, -0.35691458]\n",
       "\n",
       "[42960 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e92437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       0.88      0.94      0.91     17341\n",
      "          ru       0.84      0.46      0.59     10379\n",
      "          en       0.96      0.82      0.89     12964\n",
      "       other       0.64      0.87      0.73     15084\n",
      "\n",
      "    accuracy                           0.80     55768\n",
      "   macro avg       0.83      0.77      0.78     55768\n",
      "weighted avg       0.83      0.80      0.80     55768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2135630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta.to_csv(\"clstm_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19925960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
