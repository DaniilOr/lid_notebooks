{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d840060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f102fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f07e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[meta[\"locale\"] != \"kz\", \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" +  meta.loc[meta[\"locale\"] != \"kz\"][\"locale\"] + \"/clips/\" + meta.loc[meta[\"locale\"] != \"kz\"][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n",
    "workdir = \"/tf/datasets/gru\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66d9c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-04 12:42:20.243 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.900, 1.100]\n",
      "2021-06-04 12:42:20.273 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-04 12:42:20.289 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-04 12:42:20.706 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-04 12:42:20.722 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-04 12:42:21.115 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-04 12:42:21.131 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.9, max=1.1, flag=None)\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "            .prefetch(1)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88234ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bea00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BGRU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, 40)]        0         \n",
      "_________________________________________________________________\n",
      "channel_dropout_0.80 (Spatia (None, None, 40)          0         \n",
      "_________________________________________________________________\n",
      "BGRU_1 (Bidirectional)       (None, None, 1024)        1701888   \n",
      "_________________________________________________________________\n",
      "BGRU_2 (Bidirectional)       (None, 1024)              4724736   \n",
      "_________________________________________________________________\n",
      "BGRU_2_bn (BatchNormalizatio (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "fc_relu_1 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "fc_relu_1_bn (BatchNormaliza (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "fc_relu_2 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "fc_relu_2_bn (BatchNormaliza (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 4100      \n",
      "_________________________________________________________________\n",
      "log_softmax (Activation)     (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 8,542,212\n",
      "Trainable params: 8,536,068\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import lidbox.models.bi_gru as bi_gru\n",
    "model_input_type = \"logmelspec\"\n",
    "\n",
    "def create_model(num_freq_bins, num_labels):\n",
    "    model = bi_gru.create([None, num_freq_bins], num_labels, channel_dropout_rate=0.8)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "        metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    num_freq_bins=20 if model_input_type == \"mfcc\" else 40,\n",
    "    num_labels=4)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9776c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407b39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing datasets\n",
      "start training\n",
      "Epoch 1/100\n",
      "2576/2576 [==============================] - 613s 237ms/step - loss: 1.1631 - sparse_categorical_accuracy: 0.4914 - val_loss: 2.1068 - val_sparse_categorical_accuracy: 0.2162\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.42752\n",
      "Epoch 2/100\n",
      "2576/2576 [==============================] - 616s 239ms/step - loss: 1.0235 - sparse_categorical_accuracy: 0.5410 - val_loss: 2.1966 - val_sparse_categorical_accuracy: 0.2644\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.42752\n",
      "Epoch 3/100\n",
      "2576/2576 [==============================] - 603s 234ms/step - loss: 0.9361 - sparse_categorical_accuracy: 0.5829 - val_loss: 1.4075 - val_sparse_categorical_accuracy: 0.4342\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.42752 to 1.40748, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 4/100\n",
      "2576/2576 [==============================] - 625s 243ms/step - loss: 0.8539 - sparse_categorical_accuracy: 0.6311 - val_loss: 1.3580 - val_sparse_categorical_accuracy: 0.4669\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.40748 to 1.35796, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 5/100\n",
      "2576/2576 [==============================] - 660s 256ms/step - loss: 0.7749 - sparse_categorical_accuracy: 0.6725 - val_loss: 1.6944 - val_sparse_categorical_accuracy: 0.4350\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.35796\n",
      "Epoch 6/100\n",
      "2576/2576 [==============================] - 659s 256ms/step - loss: 0.7236 - sparse_categorical_accuracy: 0.6955 - val_loss: 1.1147 - val_sparse_categorical_accuracy: 0.5318\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.35796 to 1.11465, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 7/100\n",
      "2576/2576 [==============================] - 653s 253ms/step - loss: 0.6934 - sparse_categorical_accuracy: 0.7117 - val_loss: 0.8124 - val_sparse_categorical_accuracy: 0.6559\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.11465 to 0.81236, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 8/100\n",
      "2576/2576 [==============================] - 636s 247ms/step - loss: 0.6537 - sparse_categorical_accuracy: 0.7300 - val_loss: 0.9953 - val_sparse_categorical_accuracy: 0.5818\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.81236\n",
      "Epoch 9/100\n",
      "2576/2576 [==============================] - 672s 261ms/step - loss: 0.6261 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.9396 - val_sparse_categorical_accuracy: 0.6396\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.81236\n",
      "Epoch 10/100\n",
      "2576/2576 [==============================] - 655s 254ms/step - loss: 0.5971 - sparse_categorical_accuracy: 0.7539 - val_loss: 1.1232 - val_sparse_categorical_accuracy: 0.5773\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.81236\n",
      "Epoch 11/100\n",
      "2576/2576 [==============================] - 670s 260ms/step - loss: 0.5869 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.7367 - val_sparse_categorical_accuracy: 0.6968\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.81236 to 0.73672, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 12/100\n",
      "2576/2576 [==============================] - 682s 265ms/step - loss: 0.5657 - sparse_categorical_accuracy: 0.7694 - val_loss: 0.6553 - val_sparse_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.73672 to 0.65525, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 13/100\n",
      "2576/2576 [==============================] - 682s 265ms/step - loss: 0.5396 - sparse_categorical_accuracy: 0.7809 - val_loss: 0.5645 - val_sparse_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.65525 to 0.56452, saving model to /tf/datasets/gru/cache/model/BGRU\n",
      "Epoch 14/100\n",
      "2576/2576 [==============================] - 646s 251ms/step - loss: 0.5290 - sparse_categorical_accuracy: 0.7858 - val_loss: 0.6964 - val_sparse_categorical_accuracy: 0.7154\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.56452\n",
      "Epoch 15/100\n",
      "2576/2576 [==============================] - 636s 247ms/step - loss: 0.5085 - sparse_categorical_accuracy: 0.7941 - val_loss: 0.7136 - val_sparse_categorical_accuracy: 0.7097\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.56452\n",
      "Epoch 16/100\n",
      "2576/2576 [==============================] - 660s 256ms/step - loss: 0.5026 - sparse_categorical_accuracy: 0.7978 - val_loss: 0.6269 - val_sparse_categorical_accuracy: 0.7525\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.56452\n",
      "Epoch 17/100\n",
      "2576/2576 [==============================] - 674s 262ms/step - loss: 0.4879 - sparse_categorical_accuracy: 0.8028 - val_loss: 0.5672 - val_sparse_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.56452\n",
      "Epoch 18/100\n",
      "2576/2576 [==============================] - 660s 256ms/step - loss: 0.4741 - sparse_categorical_accuracy: 0.8097 - val_loss: 0.7016 - val_sparse_categorical_accuracy: 0.7023\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.56452\n"
     ]
    }
   ],
   "source": [
    "print(\"preparing datasets\")\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input)\n",
    "dev_ds = split2ds[\"dev\"].map(as_model_input)\n",
    "\n",
    " \n",
    "print(\"start training\")    \n",
    "with tf.device(\"GPU\"):\n",
    "    history = model.fit(\n",
    "        train_ds.batch(32).repeat(100),\n",
    "        steps_per_epoch=2576,\n",
    "        validation_data=dev_ds.batch(32).repeat(100),\n",
    "        validation_steps=961,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        epochs=100)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model, \"gru.h5\", overwrite=True, include_optimizer=True, save_format=None,\n",
    "    signatures=None, options=None, save_traces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2297ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_dataframe(ids, predictions):\n",
    "    return (pd.DataFrame.from_dict({\"id\": ids, \"prediction\": predictions})\n",
    "            #.set_index(\"id\", drop=True, verify_integrity=True)\n",
    "            #.sort_index()\n",
    "           )\n",
    "\n",
    "def predict_with_model(model, ds, predict_fn=None):\n",
    "    \"\"\"\n",
    "    Map callable model over all batches in ds, predicting values for each element at key 'input'.\n",
    "    \"\"\"\n",
    "    if predict_fn is None:\n",
    "        def predict_fn(x):\n",
    "            with tf.device(\"GPU\"):\n",
    "                return x[\"id\"], model(x[\"input\"], training=False)\n",
    "\n",
    "    ids = []\n",
    "    predictions = []\n",
    "    for id, pred in ds.map(predict_fn, num_parallel_calls=TF_AUTOTUNE).unbatch().as_numpy_iterator():\n",
    "        ids.append(id.decode(\"utf-8\"))\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return predictions_to_dataframe(ids, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5128397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = predict_with_model(\n",
    "    model=model,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    #predict_fn=predict_with_ap_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad3202e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71684-000001</th>\n",
       "      <td>[-6.6333766, -1.7371142, -1.8038024, -0.418573]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71684-000002</th>\n",
       "      <td>[-11.755868, -2.9275784, -0.9396695, -0.5875112]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71684-000003</th>\n",
       "      <td>[-13.905518, -1.6065432, -0.9924134, -0.8469087]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71684-000004</th>\n",
       "      <td>[-11.246083, -2.9696379, -0.882289, -0.62580544]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88574-000001</th>\n",
       "      <td>[-3.2370036, -1.7754825, -3.876099, -0.2606049]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259-000003</th>\n",
       "      <td>[-15.581085, -0.47045177, -8.218401, -0.980802]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000001</th>\n",
       "      <td>[-12.51034, -3.3660367, -2.6351688, -0.112314366]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000002</th>\n",
       "      <td>[-13.431554, -2.9179392, -2.675808, -0.13113156]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000003</th>\n",
       "      <td>[-12.88188, -3.1726012, -3.3085423, -0.08171614]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12245-000004</th>\n",
       "      <td>[-11.041064, -2.2727833, -2.8598173, -0.17473483]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175438 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     prediction\n",
       "id                                                             \n",
       "71684-000001    [-6.6333766, -1.7371142, -1.8038024, -0.418573]\n",
       "71684-000002   [-11.755868, -2.9275784, -0.9396695, -0.5875112]\n",
       "71684-000003   [-13.905518, -1.6065432, -0.9924134, -0.8469087]\n",
       "71684-000004   [-11.246083, -2.9696379, -0.882289, -0.62580544]\n",
       "88574-000001    [-3.2370036, -1.7754825, -3.876099, -0.2606049]\n",
       "...                                                         ...\n",
       "259-000003      [-15.581085, -0.47045177, -8.218401, -0.980802]\n",
       "12245-000001  [-12.51034, -3.3660367, -2.6351688, -0.112314366]\n",
       "12245-000002   [-13.431554, -2.9179392, -2.675808, -0.13113156]\n",
       "12245-000003   [-12.88188, -3.1726012, -3.3085423, -0.08171614]\n",
       "12245-000004  [-11.041064, -2.2727833, -2.8598173, -0.17473483]\n",
       "\n",
       "[175438 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f563998",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2pred = chunk2pred.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06484933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-9.260615, -2.600781, -3.7446494, -0.2280062]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-10.804147, -2.717781, -3.5896728, -0.28567746]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[-14.39197, -0.8050288, -3.9227448, -0.8455879]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[-9.586703, -1.5442146, -4.1786733, -0.64503604]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>[-15.005293, -4.760601, -2.2678008, -0.118896626]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>[-0.6190233, -1.7051761, -7.9411693, -1.3518697]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>[-1.4948798, -0.9733737, -8.4881315, -1.2547462]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>[-0.4074719, -3.2042804, -5.582465, -1.2688036]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>[-0.8040076, -1.6110811, -7.738201, -1.083901]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[-11.872587, -0.91614926, -7.246687, -0.5645319]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42960 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prediction\n",
       "id                                                      \n",
       "0         [-9.260615, -2.600781, -3.7446494, -0.2280062]\n",
       "1       [-10.804147, -2.717781, -3.5896728, -0.28567746]\n",
       "100      [-14.39197, -0.8050288, -3.9227448, -0.8455879]\n",
       "1000    [-9.586703, -1.5442146, -4.1786733, -0.64503604]\n",
       "10000  [-15.005293, -4.760601, -2.2678008, -0.118896626]\n",
       "...                                                  ...\n",
       "99974   [-0.6190233, -1.7051761, -7.9411693, -1.3518697]\n",
       "99983   [-1.4948798, -0.9733737, -8.4881315, -1.2547462]\n",
       "99986    [-0.4074719, -3.2042804, -5.582465, -1.2688036]\n",
       "99987     [-0.8040076, -1.6110811, -7.738201, -1.083901]\n",
       "9999    [-11.872587, -0.91614926, -7.246687, -0.5645319]\n",
       "\n",
       "[42960 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c934bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       0.99      0.64      0.77     17341\n",
      "          ru       0.74      0.46      0.57     10379\n",
      "          en       0.98      0.35      0.51     12964\n",
      "       other       0.41      0.92      0.57     15084\n",
      "\n",
      "    accuracy                           0.61     55768\n",
      "   macro avg       0.78      0.59      0.61     55768\n",
      "weighted avg       0.78      0.61      0.62     55768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d36dfd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>locale</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[-9.260615, -2.600781, -3.7446494, -0.2280062]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ky</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>[-9.260615, -2.600781, -3.7446494, -0.2280062]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>[-9.260615, -2.600781, -3.7446494, -0.2280062]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ta</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>[-10.804147, -2.717781, -3.5896728, -0.28567746]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ky</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>[-10.804147, -2.717781, -3.5896728, -0.28567746]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>99974</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6190233, -1.7051761, -7.9411693, -1.3518697]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>99983</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.4948798, -0.9733737, -8.4881315, -1.2547462]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99986</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.4074719, -3.2042804, -5.582465, -1.2688036]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99987</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8040076, -1.6110811, -7.738201, -1.083901]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[-11.872587, -0.91614926, -7.246687, -0.5645319]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55768 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               path locale  \\\n",
       "id                                                                            \n",
       "0               0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "0               0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ky   \n",
       "0               0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  zh-CN   \n",
       "1               1  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ta   \n",
       "1               1  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ky   \n",
       "...           ...                                                ...    ...   \n",
       "99974       99974  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "99983       99983  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "99986       99986  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "99987       99987  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "9999         9999  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "\n",
       "      split  target                                        prediction  \n",
       "id                                                                     \n",
       "0      test       1    [-9.260615, -2.600781, -3.7446494, -0.2280062]  \n",
       "0      test       3    [-9.260615, -2.600781, -3.7446494, -0.2280062]  \n",
       "0      test       3    [-9.260615, -2.600781, -3.7446494, -0.2280062]  \n",
       "1      test       3  [-10.804147, -2.717781, -3.5896728, -0.28567746]  \n",
       "1      test       3  [-10.804147, -2.717781, -3.5896728, -0.28567746]  \n",
       "...     ...     ...                                               ...  \n",
       "99974  test       0  [-0.6190233, -1.7051761, -7.9411693, -1.3518697]  \n",
       "99983  test       0  [-1.4948798, -0.9733737, -8.4881315, -1.2547462]  \n",
       "99986  test       0   [-0.4074719, -3.2042804, -5.582465, -1.2688036]  \n",
       "99987  test       0    [-0.8040076, -1.6110811, -7.738201, -1.083901]  \n",
       "9999   test       1  [-11.872587, -0.91614926, -7.246687, -0.5645319]  \n",
       "\n",
       "[55768 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc529227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
