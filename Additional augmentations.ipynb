{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bb277a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Automatically reload imported modules that are changed outside this notebook\n",
    "# More pixels in figures\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0243e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9657b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some preprocessing to make sure that the path is correct\n",
    "meta.loc[meta[\"locale\"] != \"kz\", \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" +  meta.loc[meta[\"locale\"] != \"kz\"][\"locale\"] + \"/clips/\" + meta.loc[meta[\"locale\"] != \"kz\"][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n",
    "workdir = \"/tf/datasets/augmentexCLSTM/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4cb2ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"id\"] = meta[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0ce61e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>locale</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71684</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88574</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17681</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>544</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ta</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96896</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55763</th>\n",
       "      <td>6225</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55764</th>\n",
       "      <td>563649</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55765</th>\n",
       "      <td>12538</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>kz</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55766</th>\n",
       "      <td>259</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>ru</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55767</th>\n",
       "      <td>12245</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>es</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55768 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               path locale  \\\n",
       "0           71684  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     en   \n",
       "1           88574  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "2           17681  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "3             544  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ta   \n",
       "4           96896  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "...           ...                                                ...    ...   \n",
       "55763        6225  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "55764      563649  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     en   \n",
       "55765       12538  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     kz   \n",
       "55766         259  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     ru   \n",
       "55767       12245  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...     es   \n",
       "\n",
       "      split  target                                                 id  \n",
       "0      test       2  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "1      test       0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "2      test       0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "3      test       3  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "4      test       0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "...     ...     ...                                                ...  \n",
       "55763  test       1  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "55764  test       2  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "55765  test       0  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "55766  test       1  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "55767  test       3  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "\n",
       "[55768 rows x 6 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.loc[meta[\"split\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "41a7299c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/ru/clips/common_voice_ru_19559882.mp3'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.iloc[0][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0be36765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a8c66d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 08:37:40.069 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.500, 1.500]\n",
      "2021-06-14 08:37:40.094 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:37:40.107 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-14 08:37:40.596 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:37:40.609 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-14 08:37:40.978 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:37:40.991 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "# preprocessing of audios\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "# reading and normalizing data\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "# augmentations using random filtering\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "# significant speed change\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.5, max=1.5, flag=None)\n",
    "\n",
    "# spliting the audio on small chunks\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(S)\n",
    "        mfccs = mfccs[...,1:21]\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "        mfccs_cmvn = cmvn(mfccs)\n",
    "\n",
    "        #S = tfio.audio.freq_mask(S, param=10)\n",
    "        #S = tfio.audio.time_mask(S, param=10)\n",
    "    return dict(x, logmelspec=S, mfccs=mfccs)\n",
    "\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "           #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(32)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(32)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(32)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "051bcf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are all blocks used to build the model. Retrieved from: https://github.com/py-lidbox/lidbox \n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Layer,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def frame_layer(filters, kernel_size, strides, padding=\"causal\", activation=\"relu\", name=\"frame\"):\n",
    "    return Conv1D(filters, kernel_size, strides, padding=padding, activation=activation, name=name)\n",
    "\n",
    "\n",
    "def segment_layer(units, activation=\"relu\", name=\"segment\"):\n",
    "    return Dense(units, activation=activation, name=name)\n",
    "class GlobalMeanStddevPooling1D(Layer):\n",
    "    \"\"\"\n",
    "    Compute arithmetic mean and standard deviation of the inputs along the time steps dimension,\n",
    "    then output the concatenation of the computed stats.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        means = tf.math.reduce_mean(inputs, axis=TIME_AXIS, keepdims=True)\n",
    "        variances = tf.math.reduce_mean(tf.math.square(inputs - means), axis=TIME_AXIS)\n",
    "        means = tf.squeeze(means, TIME_AXIS)\n",
    "        stddevs = tf.math.sqrt(tf.clip_by_value(variances, STDDEV_SQRT_MIN_CLIP, variances.dtype.max))\n",
    "        return tf.concat((means, stddevs), axis=TIME_AXIS)\n",
    "\n",
    "def as_embedding_extractor(m):\n",
    "    l = m.get_layer(name=\"segment1\")\n",
    "    l.activation = None\n",
    "    return Model(inputs=m.inputs, outputs=l.output)\n",
    "\n",
    "def frequency_attention(H, d_a=64, d_f=16):\n",
    "    assert not H.shape[2] % d_f, \"amount of frequency channels ({}) must be evenly divisible by the amount of frequency attention bins (d_f={})\".format(H.shape[2], d_f)\n",
    "    # Note, we assume that H.shape = (batch_size, T, d_h), but the paper assumes the timesteps come last\n",
    "    x = Dense(d_a, activation=\"relu\", use_bias=False, name=\"Wf_1\")(H)\n",
    "    F_A = Dense(d_f, activation=\"softmax\", use_bias=False, name=\"Wf_2\")(x)\n",
    "    # Apply frequency attention on d_f bins\n",
    "    F_A = Reshape((F_A.shape[1] or -1, F_A.shape[2], 1), name=\"expand_bin_weight_dim\")(F_A)\n",
    "    H_bins = Reshape((H.shape[1] or -1, d_f, H.shape[2] // d_f), name=\"partition_freq_bins\")(H)\n",
    "    H_bins = Multiply(name=\"freq_attention\")([F_A, H_bins])\n",
    "    # Merge weighted frequency bins\n",
    "    H_weighted = Reshape((H.shape[1] or -1, H.shape[2]), name=\"merge_weighted_bins\")(H_bins)\n",
    "    return H_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb63be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    GaussianNoise,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LSTM,\n",
    "    Multiply,\n",
    "    Reshape,\n",
    ")\n",
    "# Assuming spectral features (Batch, Time, Channels), where freq. channels are always last\n",
    "TIME_AXIS = 1\n",
    "STDDEV_SQRT_MIN_CLIP = 1e-10\n",
    "def frequency_attention(H, d_a=64, d_f=16):\n",
    "    assert not H.shape[2] % d_f, \"amount of frequency channels ({}) must be evenly divisible by the amount of frequency attention bins (d_f={})\".format(H.shape[2], d_f)\n",
    "    # Note, we assume that H.shape = (batch_size, T, d_h), but the paper assumes the timesteps come last\n",
    "    x = Dense(d_a, activation=\"relu\", use_bias=False, name=\"Wf_1\")(H)\n",
    "    F_A = Dense(d_f, activation=\"softmax\", use_bias=False, name=\"Wf_2\")(x)\n",
    "    # Apply frequency attention on d_f bins\n",
    "    F_A = Reshape((F_A.shape[1] or -1, F_A.shape[2], 1), name=\"expand_bin_weight_dim\")(F_A)\n",
    "    H_bins = Reshape((H.shape[1] or -1, d_f, H.shape[2] // d_f), name=\"partition_freq_bins\")(H)\n",
    "    H_bins = Multiply(name=\"freq_attention\")([F_A, H_bins])\n",
    "    # Merge weighted frequency bins\n",
    "    H_weighted = Reshape((H.shape[1] or -1, H.shape[2]), name=\"merge_weighted_bins\")(H_bins)\n",
    "    return H_weighted\n",
    "\n",
    "\n",
    "def create(input_shape, num_outputs, output_activation=\"log_softmax\", use_attention=False, use_conv2d=False, use_lstm=False):\n",
    "    inputs = Input(shape=input_shape, name=\"input\")\n",
    "    x = inputs\n",
    "    x = GaussianNoise(stddev=0.15, name=\"input_noise\")(x)\n",
    "    x = SpatialDropout1D(0.8, name=\"channel_dropout\")(x)\n",
    "\n",
    "    if use_conv2d:\n",
    "        x = Reshape((input_shape[0] or -1, input_shape[1], 1), name=\"reshape_to_image\")(x)\n",
    "        x = Conv2D(128, (3, 9), (1, 6), activation=None, padding=\"same\", name=\"conv2d_1\")(x)\n",
    "        x = BatchNormalization(name=\"conv2d_1_bn\")(x)\n",
    "        x = Activation(\"relu\", name=\"conv2d_1_relu\")(x)\n",
    "        x = Conv2D(256, (3, 9), (1, 6), activation=None, padding=\"same\", name=\"conv2d_2\")(x)\n",
    "        x = BatchNormalization(name=\"conv2d_2_bn\")(x)\n",
    "        x = Activation(\"relu\", name=\"conv2d_2_relu\")(x)\n",
    "        # x = Reshape((x.shape[1] or -1, x.shape[2] * x.shape[3]), name=\"flatten_image_channels\")(x)\n",
    "        x = tf.math.reduce_max(x, axis=2, name=\"maxpool_image_channels\")\n",
    "    \n",
    "    x = Dropout(rate=0.7, name=\"dropout1\")(x)\n",
    "    x = frame_layer(512, 5, 1, name=\"frame1\")(x)\n",
    "    x = frame_layer(512, 3, 2, name=\"frame2\")(x)\n",
    "    x = frame_layer(512, 3, 3, name=\"frame3\")(x)\n",
    "    x = Dropout(rate=0.6, name=\"dropout2\")(x)\n",
    "\n",
    "    if use_lstm:\n",
    "        x = LSTM(512, name=\"lstm\", return_sequences=True)(x)\n",
    "    \n",
    "    x = frame_layer(512, 1, 1, name=\"frame4\")(x)\n",
    "    x = frame_layer(1500, 1, 1, name=\"frame5\")(x)\n",
    "    if use_attention:\n",
    "        x = frequency_attention(x, d_f=60)\n",
    "\n",
    "    x = GlobalMeanStddevPooling1D(name=\"stats_pooling\")(x)\n",
    "    x = Dropout(rate=0.5, name=\"dropout3\")(x)\n",
    "\n",
    "    x = segment_layer(512, name=\"segment1\")(x)\n",
    "    x = segment_layer(512, name=\"segment2\")(x)\n",
    "    outputs = Dense(num_outputs, name=\"output\", activation=None)(x)\n",
    "\n",
    "    if output_activation:\n",
    "        outputs = Activation(getattr(tf.nn, output_activation), name=str(output_activation))(outputs)\n",
    "    return Model(inputs=inputs, outputs=outputs, name=\"CLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a063f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3075040e0644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_freq_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     m = create(\n\u001b[1;32m      3\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_freq_bins\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0muse_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_conv2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         num_outputs=num_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def create_model(num_freq_bins=40, num_labels=len(np.unique(meta.target))):\n",
    "    m = create(\n",
    "        input_shape=[None, num_freq_bins],\n",
    "        use_attention=True, use_conv2d=True, use_lstm=True,\n",
    "        num_outputs=num_labels)\n",
    "    m.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "            metrics=tf.keras.metrics.sparse_categorical_accuracy)\n",
    "    return m\n",
    "\n",
    "with tf.device(\"GPU\"):\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "   \n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(cachedir, \"tensorboard\", model.name),\n",
    "        update_freq=\"epoch\",\n",
    "        write_images=True,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(cachedir, \"model\", model.name),\n",
    "        monitor='val_loss',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def as_model_input(x):\n",
    "    return x[\"logmelspec\"], x[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda790cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing datasets\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2576/2576 [==============================] - 108s 41ms/step - loss: 1.2638 - sparse_categorical_accuracy: 0.4379 - val_loss: 1.6157 - val_sparse_categorical_accuracy: 0.1885\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61568, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 2/100\n",
      "2576/2576 [==============================] - 103s 40ms/step - loss: 1.0887 - sparse_categorical_accuracy: 0.4997 - val_loss: 2.0624 - val_sparse_categorical_accuracy: 0.2501\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.61568\n",
      "Epoch 3/100\n",
      "2576/2576 [==============================] - 103s 40ms/step - loss: 0.9876 - sparse_categorical_accuracy: 0.5354 - val_loss: 1.6566 - val_sparse_categorical_accuracy: 0.3071\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.61568\n",
      "Epoch 4/100\n",
      "2576/2576 [==============================] - 106s 41ms/step - loss: 0.9250 - sparse_categorical_accuracy: 0.5656 - val_loss: 1.4514 - val_sparse_categorical_accuracy: 0.3771\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.61568 to 1.45143, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 5/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.8797 - sparse_categorical_accuracy: 0.5858 - val_loss: 1.6649 - val_sparse_categorical_accuracy: 0.3513\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.45143\n",
      "Epoch 6/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.8474 - sparse_categorical_accuracy: 0.6014 - val_loss: 1.4957 - val_sparse_categorical_accuracy: 0.4067\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.45143\n",
      "Epoch 7/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.8185 - sparse_categorical_accuracy: 0.6242 - val_loss: 1.4731 - val_sparse_categorical_accuracy: 0.4095\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.45143\n",
      "Epoch 8/100\n",
      "2576/2576 [==============================] - 105s 41ms/step - loss: 0.7830 - sparse_categorical_accuracy: 0.6487 - val_loss: 1.1972 - val_sparse_categorical_accuracy: 0.5164\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.45143 to 1.19717, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 9/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.7610 - sparse_categorical_accuracy: 0.6616 - val_loss: 1.3434 - val_sparse_categorical_accuracy: 0.4667\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.19717\n",
      "Epoch 10/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.7350 - sparse_categorical_accuracy: 0.6745 - val_loss: 1.3866 - val_sparse_categorical_accuracy: 0.4604\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.19717\n",
      "Epoch 11/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.7208 - sparse_categorical_accuracy: 0.6813 - val_loss: 1.4015 - val_sparse_categorical_accuracy: 0.4811\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.19717\n",
      "Epoch 12/100\n",
      "2576/2576 [==============================] - 105s 41ms/step - loss: 0.7084 - sparse_categorical_accuracy: 0.6900 - val_loss: 1.0814 - val_sparse_categorical_accuracy: 0.5463\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.19717 to 1.08145, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 13/100\n",
      "2576/2576 [==============================] - 102s 40ms/step - loss: 0.6944 - sparse_categorical_accuracy: 0.6982 - val_loss: 1.1076 - val_sparse_categorical_accuracy: 0.5671\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.08145\n",
      "Epoch 14/100\n",
      "2576/2576 [==============================] - 102s 39ms/step - loss: 0.6798 - sparse_categorical_accuracy: 0.7042 - val_loss: 1.3618 - val_sparse_categorical_accuracy: 0.4666\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.08145\n",
      "Epoch 15/100\n",
      "2576/2576 [==============================] - 102s 39ms/step - loss: 0.6687 - sparse_categorical_accuracy: 0.7112 - val_loss: 1.1276 - val_sparse_categorical_accuracy: 0.5191\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.08145\n",
      "Epoch 16/100\n",
      "2576/2576 [==============================] - 104s 40ms/step - loss: 0.6614 - sparse_categorical_accuracy: 0.7128 - val_loss: 0.9535 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.08145 to 0.95353, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 17/100\n",
      "2576/2576 [==============================] - 107s 42ms/step - loss: 0.6489 - sparse_categorical_accuracy: 0.7198 - val_loss: 0.8700 - val_sparse_categorical_accuracy: 0.6146\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.95353 to 0.86996, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 18/100\n",
      "2576/2576 [==============================] - 103s 40ms/step - loss: 0.6421 - sparse_categorical_accuracy: 0.7241 - val_loss: 1.0799 - val_sparse_categorical_accuracy: 0.5592\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.86996\n",
      "Epoch 19/100\n",
      "2576/2576 [==============================] - 110s 43ms/step - loss: 0.6321 - sparse_categorical_accuracy: 0.7281 - val_loss: 0.8675 - val_sparse_categorical_accuracy: 0.6162\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.86996 to 0.86749, saving model to /tf/datasets/augmentexCLSTM/cache/model/CLSTM\n",
      "Epoch 20/100\n",
      "1228/2576 [=============>................] - ETA: 48s - loss: 0.6131 - sparse_categorical_accuracy: 0.7383"
     ]
    }
   ],
   "source": [
    "print(\"preparing datasets\")\n",
    "\n",
    "train_ds = split2ds[\"train\"].map(as_model_input)\n",
    "dev_ds = split2ds[\"dev\"].map(as_model_input)\n",
    "\n",
    "# training\n",
    "print(\"start training\")    \n",
    "with tf.device(\"GPU\"):\n",
    "    history = model.fit(\n",
    "        train_ds.batch(32).repeat(100),\n",
    "        steps_per_epoch=2576,\n",
    "        validation_data=dev_ds.batch(32).repeat(100),\n",
    "        validation_steps=961,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        epochs=100)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model, \"augmentedCLSTM.h5\", overwrite=True, include_optimizer=True, save_format=None,\n",
    "    signatures=None, options=None, save_traces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f3c4f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/datasets/augmentexCLSTM/cache/model/CLSTM\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lidbox.util import predict_with_model, classification_report\n",
    "from lidbox.visualize import draw_confusion_matrix\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    model = create_model()\n",
    "    print(os.path.join(cachedir, \"model\", model.name))\n",
    "    model.load_weights(os.path.join(cachedir, \"model\", model.name))\n",
    "    return model\n",
    "\n",
    "\n",
    "def display_classification_report(report):\n",
    "    for m in (\"avg_detection_cost\", \"avg_equal_error_rate\", \"accuracy\"):\n",
    "        print(\"{}: {:.3f}\".format(m, report[m]))\n",
    "\n",
    "    lang_metrics = pd.DataFrame.from_dict(\n",
    "        {k: v for k, v in report.items() if k in lang2target})\n",
    "    lang_metrics[\"mean\"] = lang_metrics.mean(axis=1)\n",
    "    display(lang_metrics.T)\n",
    "\n",
    "    fig, ax = draw_confusion_matrix(report[\"confusion_matrix\"], lang2target)\n",
    "\n",
    "model = load_trained_model()\n",
    "\n",
    "def predict_with_ap_loss(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        # Generate language vector for input spectra\n",
    "        language_vector = model(x[\"input\"], training=False)\n",
    "        print(language_vector)\n",
    "        # Predict languages by computing distances to reference directions\n",
    "        return x[\"id\"], model.loss.predict(language_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96dd90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6feae428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on the source domain\n",
    "chunk2pred = predict_with_model(\n",
    "    model=model,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    #predict_fn=predict_with_ap_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16809c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424383.mp3-000001</th>\n",
       "      <td>[-6.447608, -0.86175203, -2.5532148, -0.6968273]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424383.mp3-000002</th>\n",
       "      <td>[-5.3443165, -0.9268003, -2.1176183, -0.73586935]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424383.mp3-000003</th>\n",
       "      <td>[-5.570038, -1.1016388, -2.1237652, -0.60828584]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424383.mp3-000004</th>\n",
       "      <td>[-5.282734, -1.0610301, -2.595479, -0.5547562]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424555.mp3-000001</th>\n",
       "      <td>[-4.649946, -1.0795896, -2.7285242, -0.5354816]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22242819.mp3-000004</th>\n",
       "      <td>[-1.519991, -2.1595426, -1.8823164, -0.66617185]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243431.mp3-000001</th>\n",
       "      <td>[-3.2410185, -2.0392914, -1.8907969, -0.38595134]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243666.mp3-000001</th>\n",
       "      <td>[-3.3413835, -1.7297513, -1.8790188, -0.4548493]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243666.mp3-000002</th>\n",
       "      <td>[-3.681122, -1.5264689, -2.3076472, -0.4185373]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22246300.mp3-000001</th>\n",
       "      <td>[-3.2555509, -2.1469824, -2.0914047, -0.3269903]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175438 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           prediction\n",
       "id                                                                                                   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-6.447608, -0.86175203, -2.5532148, -0.6968273]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [-5.3443165, -0.9268003, -2.1176183, -0.73586935]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-5.570038, -1.1016388, -2.1237652, -0.60828584]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...     [-5.282734, -1.0610301, -2.595479, -0.5547562]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...    [-4.649946, -1.0795896, -2.7285242, -0.5354816]\n",
       "...                                                                                               ...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-1.519991, -2.1595426, -1.8823164, -0.66617185]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [-3.2410185, -2.0392914, -1.8907969, -0.38595134]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-3.3413835, -1.7297513, -1.8790188, -0.4548493]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...    [-3.681122, -1.5264689, -2.3076472, -0.4185373]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-3.2555509, -2.1469824, -2.0914047, -0.3269903]\n",
       "\n",
       "[175438 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk2pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd9a4521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424383.mp3</th>\n",
       "      <td>[-5.661174, -0.9878053, -2.3475194, -0.64893466]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424555.mp3</th>\n",
       "      <td>[-5.1115446, -0.9818246, -2.9906769, -0.57174474]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424567.mp3</th>\n",
       "      <td>[-3.223493, -1.5270684, -2.5562506, -0.40734017]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424609.mp3</th>\n",
       "      <td>[-3.3473876, -2.0355728, -1.8754183, -0.3842996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/cs/clips/common_voice_cs_20424636.mp3</th>\n",
       "      <td>[-3.9393935, -1.7496147, -1.5359318, -0.5489157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22242585.mp3</th>\n",
       "      <td>[-2.5344598, -2.723149, -1.8907971, -0.35086846]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22242819.mp3</th>\n",
       "      <td>[-1.7739997, -2.8186498, -1.3505759, -0.71904624]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243431.mp3</th>\n",
       "      <td>[-3.2410185, -2.0392914, -1.8907969, -0.38595134]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22243666.mp3</th>\n",
       "      <td>[-3.5112529, -1.6281102, -2.093333, -0.4366933]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/zh-CN/clips/common_voice_zh-CN_22246300.mp3</th>\n",
       "      <td>[-3.2555509, -2.1469824, -2.0914047, -0.3269903]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55768 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           prediction\n",
       "id                                                                                                   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-5.661174, -0.9878053, -2.3475194, -0.64893466]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [-5.1115446, -0.9818246, -2.9906769, -0.57174474]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-3.223493, -1.5270684, -2.5562506, -0.40734017]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-3.3473876, -2.0355728, -1.8754183, -0.3842996]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-3.9393935, -1.7496147, -1.5359318, -0.5489157]\n",
       "...                                                                                               ...\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-2.5344598, -2.723149, -1.8907971, -0.35086846]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [-1.7739997, -2.8186498, -1.3505759, -0.71904624]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  [-3.2410185, -2.0392914, -1.8907969, -0.38595134]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...    [-3.5112529, -1.6281102, -2.093333, -0.4366933]\n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...   [-3.2555509, -2.1469824, -2.0914047, -0.3269903]\n",
       "\n",
       "[55768 rows x 1 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b80f023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       0.99      0.74      0.84     17341\n",
      "          ru       0.66      0.39      0.49     10379\n",
      "          en       0.83      0.32      0.47     12964\n",
      "       other       0.42      0.88      0.57     15084\n",
      "\n",
      "    accuracy                           0.62     55768\n",
      "   macro avg       0.72      0.58      0.59     55768\n",
      "weighted avg       0.74      0.62      0.62     55768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39288d",
   "metadata": {},
   "source": [
    "## VOX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "996b34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the code is similar to the one used before, but it is used to load the target domain\n",
    "import numpy as np\n",
    "np_rng = np.random.default_rng(1)\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "from lidbox.meta import (\n",
    "    common_voice,\n",
    "    generate_label2target,\n",
    "    verify_integrity,\n",
    "    read_audio_durations,\n",
    "    random_oversampling_on_split\n",
    ")\n",
    "\n",
    "tf.random.set_seed(np_rng.integers(0, tf.int64.max))\n",
    "\n",
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"new_test.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"new_dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "train[\"path\"] = train[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "test[\"path\"] = test[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "dev[\"path\"] = dev[\"path\"].apply(lambda x: x[:-3] + \"mp3\")\n",
    "\n",
    "train[\"split\"] = \"train\"\n",
    "test[\"split\"] = \"test\"\n",
    "dev[\"split\"] = \"dev\"\n",
    "meta = pd.concat([train, test, dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8f23017",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[((meta[\"locale\"] != \"kz\") & ~(((meta[\"split\"] == \"dev\") | (meta[\"split\"] == \"test\")) & ((meta[\"locale\"] == \"ru\") | (meta[\"locale\"] == \"kz\") | (meta[\"locale\"] == \"en\")))), \"path\"] = \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/\" + meta.loc[((meta[\"locale\"] != \"kz\") & ~(((meta[\"split\"] == \"dev\") | (meta[\"split\"] == \"test\")) & ((meta[\"locale\"] == \"ru\") | (meta[\"locale\"] == \"kz\") | (meta[\"locale\"] == \"en\"))))][\"locale\"]  + \"/clips/\" + meta.loc[((meta[\"locale\"] != \"kz\") & ~(((meta[\"split\"] == \"dev\") | (meta[\"split\"] == \"test\")) & ((meta[\"locale\"] == \"ru\") | (meta[\"locale\"] == \"kz\") | (meta[\"locale\"] == \"en\"))))][\"path\"]\n",
    "targets = {\"kz\": 0, \"ru\": 1, \"en\":2, \"other\":3}\n",
    "meta[\"target\"] = meta[\"locale\"]\n",
    "meta.loc[(meta[\"locale\"] != \"kz\") & (meta[\"locale\"] != \"ru\") & (meta[\"locale\"]!=\"en\"), \"target\"] = \"other\"\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5f590a130a73c.mp3\"]\n",
    "meta = meta.loc[meta[\"path\"] != \"/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/kz/clips/5ef9bd9ba7029.mp3\"]\n",
    "\n",
    "meta[\"id\"] = str(meta[\"Unnamed: 0\"])\n",
    "meta[\"target\"] = meta[\"target\"].map(targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d9caf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       /tf/datasets/vox/en_test/shrDRhToGpY__U__S133-...\n",
       "1       /tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123-...\n",
       "2       /tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---...\n",
       "3       /tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---...\n",
       "4       /tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---...\n",
       "                              ...                        \n",
       "9995    /tf/datasets/vox/en_test/KLiy94kfZI4__U__S133-...\n",
       "9996    /tf/datasets/vox/en_test/YTlliEr5LOA__U__S113-...\n",
       "9997    /tf/datasets/vox/en_test/bSs0gNq6Kkc__U__S0---...\n",
       "9998    /tf/datasets/vox/en_test/Da7c-BY6MDA__U__S2---...\n",
       "9999    /tf/datasets/vox/en_test/VWvPndMo1F8__U__S24--...\n",
       "Name: path, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"ru\"), \"path\"] = meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"ru\")][\"path\"].apply(lambda x: f\"/tf/datasets/vox/ru_test/{x}\")\n",
    "meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"ru\"), \"path\"]\n",
    "meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"kz\"), \"path\"] = meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"kz\")][\"path\"].apply(lambda x: f\"/tf/datasets/vox/kz_test/{x}\")\n",
    "meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"kz\"), \"path\"] \n",
    "meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"en\"), \"path\"] = meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"en\")][\"path\"].apply(lambda x: f\"/tf/datasets/vox/en_test/{x}\")\n",
    "meta.loc[(meta[\"split\"] == \"test\") & (meta[\"locale\"] == \"en\"), \"path\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6630ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[meta[\"split\"]==\"test\", \"Unnamed: 0\"] = meta.loc[meta[\"split\"]==\"test\"][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "028f6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[\"id\"] = meta[\"Unnamed: 0\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0f1e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[meta[\"split\"] == \"test\", \"id\"] = meta.loc[meta[\"split\"] == \"test\"][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0743f14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>locale</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/shrDRhToGpY__U__S133---0944.430-0958.260.mp3</th>\n",
       "      <td>/tf/datasets/vox/en_test/shrDRhToGpY__U__S133-...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/vox/en_test/shrDRhToGpY__U__S133-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123---0427.020-0444.670.mp3</th>\n",
       "      <td>/tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123-...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---0398.760-0403.940.mp3</th>\n",
       "      <td>/tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---1473.480-1485.720.mp3</th>\n",
       "      <td>/tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---0125.230-0140.900.mp3</th>\n",
       "      <td>/tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---...</td>\n",
       "      <td>en</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>/tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/it/clips/common_voice_it_20015623.mp3</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>it</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/uk/clips/common_voice_uk_23554602.mp3</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>uk</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/tr/clips/common_voice_tr_20416266.mp3</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>tr</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/it/clips/common_voice_it_20263173.mp3</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>it</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-11/es/clips/common_voice_es_20283066.mp3</th>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "      <td>es</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>/tf/datasets/data_untar/cv-corpus-6.1-2020-12-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51137 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 path  \\\n",
       "Unnamed: 0                                                                                              \n",
       "/tf/datasets/vox/en_test/shrDRhToGpY__U__S133--...  /tf/datasets/vox/en_test/shrDRhToGpY__U__S133-...   \n",
       "/tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123--...  /tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123-...   \n",
       "/tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---0...  /tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---...   \n",
       "/tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---1...  /tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---...   \n",
       "/tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---0...  /tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---...   \n",
       "...                                                                                               ...   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...   \n",
       "\n",
       "                                                   locale split  target  \\\n",
       "Unnamed: 0                                                                \n",
       "/tf/datasets/vox/en_test/shrDRhToGpY__U__S133--...     en  test       2   \n",
       "/tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123--...     en  test       2   \n",
       "/tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---0...     en  test       2   \n",
       "/tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---1...     en  test       2   \n",
       "/tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---0...     en  test       2   \n",
       "...                                                   ...   ...     ...   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...     it  test       3   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...     uk  test       3   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...     tr  test       3   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...     it  test       3   \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...     es  test       3   \n",
       "\n",
       "                                                                                                   id  \n",
       "Unnamed: 0                                                                                             \n",
       "/tf/datasets/vox/en_test/shrDRhToGpY__U__S133--...  /tf/datasets/vox/en_test/shrDRhToGpY__U__S133-...  \n",
       "/tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123--...  /tf/datasets/vox/en_test/mzfg0RGJnV8__U__S123-...  \n",
       "/tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---0...  /tf/datasets/vox/en_test/-_PPCH3y0eE__U__S1---...  \n",
       "/tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---1...  /tf/datasets/vox/en_test/DQMxvGYyu6Q__U__S0---...  \n",
       "/tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---0...  /tf/datasets/vox/en_test/x4lfSc7PrB0__U__S0---...  \n",
       "...                                                                                               ...  \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "/tf/datasets/data_untar/cv-corpus-6.1-2020-12-1...  /tf/datasets/data_untar/cv-corpus-6.1-2020-12-...  \n",
       "\n",
       "[51137 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = meta.set_index(\"Unnamed: 0\")\n",
    "meta.loc[meta[\"split\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e03c793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.loc[meta[\"split\"] == \"test\"] = meta.loc[(meta[\"split\"] == \"test\") & (meta[\"target\"] != 3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bcddfe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 08:16:01.183 I lidbox.data.steps: Applying random resampling to signals with a random speed ratio chosen uniformly at random from [0.900, 1.100]\n",
      "2021-06-14 08:16:01.308 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:16:01.321 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-14 08:16:01.747 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:16:01.760 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-14 08:16:02.071 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:16:02.083 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n",
      "2021-06-14 08:16:02.410 I lidbox.data.steps: Repeating all signals until they are at least 3200 ms\n",
      "2021-06-14 08:16:02.423 I lidbox.data.steps: Dividing every signal in the dataset into new signals by creating signal chunks of length 3200 ms and offset 800 ms. Maximum amount of padding allowed in the last chunk is 0 ms.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "from lidbox.features import audio, cmvn\n",
    "import lidbox.data.steps as ds_steps\n",
    "\n",
    "\n",
    "TF_AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def metadata_to_dataset_input(meta):\n",
    "    return {\n",
    "        \"id\": tf.constant(meta.id, tf.string),\n",
    "        \"path\": tf.constant(meta.path, tf.string),\n",
    "        \"target\": tf.constant(meta.target, tf.int32),\n",
    "        \"split\": tf.constant(meta.split, tf.string),\n",
    "    }\n",
    "\n",
    "def read_mp3(x):\n",
    "    s, r = audio.read_mp3(x[\"path\"])\n",
    "    out_rate = 16000\n",
    "    s = audio.resample(s, r, out_rate)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    s = audio.remove_silence(s, out_rate)\n",
    "    return dict(x, signal=s, sample_rate=out_rate)\n",
    "\n",
    "\n",
    "def random_filter(x):\n",
    "    def scipy_filter(s, N=10):\n",
    "        b = np_rng.normal(0, 1, N)\n",
    "        return scipy.signal.lfilter(b, 1.0, s).astype(np.float32), b\n",
    "    s, _ = tf.numpy_function(\n",
    "        scipy_filter,\n",
    "        [x[\"signal\"]],\n",
    "        [tf.float32, tf.float64],\n",
    "        name=\"np_random_filter\")\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    s = audio.peak_normalize(s, dBFS=-3.0)\n",
    "    return dict(x, signal=s)\n",
    "\n",
    "\n",
    "def random_speed_change(ds):\n",
    "    return ds_steps.random_signal_speed_change(ds, min=0.9, max=1.1, flag=None)\n",
    "\n",
    "\n",
    "def create_signal_chunks(ds):\n",
    "    ds = ds_steps.repeat_too_short_signals(ds, 3200)\n",
    "    ds = ds_steps.create_signal_chunks(ds, 3200, 800)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def batch_extract_features(x):\n",
    "    with tf.device(\"GPU\"):\n",
    "        signals, rates = x[\"signal\"], x[\"sample_rate\"]\n",
    "        S = audio.spectrograms(signals, rates[0])\n",
    "        S = audio.linear_to_mel(S, rates[0])\n",
    "        S = tf.math.log(S + 1e-6)\n",
    "        S = cmvn(S, normalize_variance=False)\n",
    "    return dict(x, logmelspec=S)\n",
    "\n",
    "def pipeline_from_meta(data, split):\n",
    "    if split == \"train\":\n",
    "        data = data.sample(frac=1, random_state=np_rng.bit_generator)\n",
    "\n",
    "    ds = (tf.data.Dataset\n",
    "            .from_tensor_slices(metadata_to_dataset_input(data))\n",
    "            .map(read_mp3, num_parallel_calls=TF_AUTOTUNE))\n",
    "\n",
    "    if split == \"train\":\n",
    "        return (ds\n",
    "            .apply(random_speed_change)\n",
    "           #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1)\n",
    "            .map(random_filter, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch())\n",
    "    else:\n",
    "        return (ds\n",
    "            .apply(create_signal_chunks)\n",
    "            .batch(1)\n",
    "            .map(batch_extract_features, num_parallel_calls=TF_AUTOTUNE)\n",
    "            .unbatch()\n",
    "            #.cache(os.path.join(cachedir, \"data\", split))\n",
    "            .prefetch(1))\n",
    "\n",
    "\n",
    "cachedir = os.path.join(workdir, \"cache\")\n",
    "\n",
    "split2ds = {split: pipeline_from_meta(meta[meta[\"split\"]==split], split)\n",
    "            for split in meta.split.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d3b1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f920167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on target domain\n",
    "chunk2pred = predict_with_model(\n",
    "    model=model,\n",
    "    ds=split2ds[\"test\"].map(lambda x: dict(x, input=x[\"logmelspec\"])).batch(32),\n",
    "    #predict_fn=predict_with_ap_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb8d4f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3-000001</th>\n",
       "      <td>[-0.9396108, -2.60894, -1.6099796, -1.09149]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3-000002</th>\n",
       "      <td>[-1.4211943, -2.865948, -1.5590718, -0.7106717]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3-000003</th>\n",
       "      <td>[-1.3829262, -2.940194, -1.6502106, -0.68460065]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3-000004</th>\n",
       "      <td>[-1.1061298, -2.8974087, -1.758347, -0.81719756]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3-000005</th>\n",
       "      <td>[-0.87291104, -3.4695358, -1.551116, -1.0813907]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0277.000-0287.980.mp3-000010</th>\n",
       "      <td>[-0.33672518, -2.9591024, -3.0301414, -1.6835115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---0301.200-0306.760.mp3-000001</th>\n",
       "      <td>[-0.9360744, -2.0881443, -1.4914705, -1.3514011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---0301.200-0306.760.mp3-000002</th>\n",
       "      <td>[-1.7620763, -2.266904, -1.3255211, -0.7786726]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---0301.200-0306.760.mp3-000003</th>\n",
       "      <td>[-2.0707474, -2.0226097, -1.1285616, -0.87203324]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---0301.200-0306.760.mp3-000004</th>\n",
       "      <td>[-2.1532104, -2.2387192, -1.2022157, -0.74072814]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334519 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           prediction\n",
       "id                                                                                                   \n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...       [-0.9396108, -2.60894, -1.6099796, -1.09149]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...    [-1.4211943, -2.865948, -1.5590718, -0.7106717]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...   [-1.3829262, -2.940194, -1.6502106, -0.68460065]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...   [-1.1061298, -2.8974087, -1.758347, -0.81719756]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...   [-0.87291104, -3.4695358, -1.551116, -1.0813907]\n",
       "...                                                                                               ...\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [-0.33672518, -2.9591024, -3.0301414, -1.6835115]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---...   [-0.9360744, -2.0881443, -1.4914705, -1.3514011]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---...    [-1.7620763, -2.266904, -1.3255211, -0.7786726]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---...  [-2.0707474, -2.0226097, -1.1285616, -0.87203324]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---...  [-2.1532104, -2.2387192, -1.2022157, -0.74072814]\n",
       "\n",
       "[334519 rows x 1 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bd0cc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0003.940-0020.570.mp3</th>\n",
       "      <td>[-1.2432518, -2.7782032, -1.5437062, -0.98101324]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0743.730-0757.100.mp3</th>\n",
       "      <td>[-1.3749871, -3.0530126, -1.3759043, -0.8816973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100---0650.920-0661.560.mp3</th>\n",
       "      <td>[-1.144896, -2.6072266, -1.4575762, -1.047575]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100---0692.790-0704.510.mp3</th>\n",
       "      <td>[-1.05083, -2.7411377, -1.45535, -1.2267107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100---0705.010-0711.610.mp3</th>\n",
       "      <td>[-2.0527196, -1.8869089, -1.5922439, -0.7132508]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0219.180-0230.690.mp3</th>\n",
       "      <td>[-1.1549723, -2.1252553, -2.3519063, -0.9764026]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0230.690-0247.370.mp3</th>\n",
       "      <td>[-1.7505469, -1.7156585, -2.5320637, -0.85430783]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0247.370-0257.750.mp3</th>\n",
       "      <td>[-1.9622586, -2.3761766, -2.0075784, -0.92366135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---0277.000-0287.980.mp3</th>\n",
       "      <td>[-1.7208687, -1.7513126, -2.4534397, -0.88329303]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---0301.200-0306.760.mp3</th>\n",
       "      <td>[-1.7305272, -2.1540942, -1.2869422, -0.9357087]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36053 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           prediction\n",
       "id                                                                                                   \n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...  [-1.2432518, -2.7782032, -1.5437062, -0.98101324]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S0---0...   [-1.3749871, -3.0530126, -1.3759043, -0.8816973]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100--...     [-1.144896, -2.6072266, -1.4575762, -1.047575]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100--...       [-1.05083, -2.7411377, -1.45535, -1.2267107]\n",
       "/tf/datasets/vox/en_test/-BwrRlUdfEs__U__S100--...   [-2.0527196, -1.8869089, -1.5922439, -0.7132508]\n",
       "...                                                                                               ...\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...   [-1.1549723, -2.1252553, -2.3519063, -0.9764026]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [-1.7505469, -1.7156585, -2.5320637, -0.85430783]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [-1.9622586, -2.3761766, -2.0075784, -0.92366135]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S20---...  [-1.7208687, -1.7513126, -2.4534397, -0.88329303]\n",
       "/tf/datasets/vox/ru_test/ztSbqN-mPtM__U__S52---...   [-1.7305272, -2.1540942, -1.2869422, -0.9357087]\n",
       "\n",
       "[36053 rows x 1 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidbox.util import merge_chunk_predictions\n",
    "\n",
    "\n",
    "utt2pred = merge_chunk_predictions(chunk2pred)\n",
    "utt2pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a05868ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          kz       0.39      0.34      0.36     13946\n",
      "          ru       0.85      0.03      0.06     12107\n",
      "          en       0.77      0.04      0.08     10000\n",
      "       other       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.16     36053\n",
      "   macro avg       0.50      0.11      0.13     36053\n",
      "weighted avg       0.65      0.16      0.19     36053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_meta = meta[meta[\"split\"]==\"test\"].join(utt2pred, how=\"outer\")\n",
    "assert not test_meta.isna().any(axis=None), \"failed to join predictions\"\n",
    "\n",
    "true_sparse = test_meta.target.to_numpy(np.int32)\n",
    "pred_dense = np.stack(test_meta.prediction.apply(np.argmax))\n",
    "\n",
    "report = classification_report(true_sparse, pred_dense, target_names=list(targets.keys()), labels=range(4))\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc763b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
